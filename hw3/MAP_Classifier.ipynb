{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: MAP Classifier\n",
    "\n",
    "In this assignment you will implement a few of the MAP classifiers learned in class.\n",
    "\n",
    "## Read the following instructions carefully:\n",
    "\n",
    "1. This jupyter notebook contains all the step by step instructions needed for this part of the exercise.\n",
    "2. Write vectorized code whenever possible.\n",
    "3. You are responsible for the correctness of your code and should add as many tests as you see fit. Tests will not be graded nor checked.\n",
    "4. Write your functions in this notebook only.\n",
    "5. You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/) and [numpy](https://www.numpy.org/devdocs/reference/) only. \n",
    "6. Your code must run without errors. During the environment setup, you were given a specific version of `numpy` to install. Changes of the configuration we provided are at your own risk. Code that cannot run will also earn you the grade of 0.\n",
    "7. Write your own code. Cheating will not be tolerated. \n",
    "8. Submission includes this notebook and the answers to the theoretical part. Answers to qualitative questions should be written in markdown cells (with $\\LaTeX$ support).\n",
    "9. You can add additional functions.\n",
    "10. Submission: zip only the completed jupyter notebook and the PDF with your solution for the theory part. Do not include the data or any directories. Name the file `ID1_ID2.zip` and submit **only one copy of the assignment**.\n",
    "\n",
    "## In this exercise you will perform the following:\n",
    "1. Implement a Naive Bayes Classifier based on Multi-Normal distribution\n",
    "1. Implement a Full Bayes Classifier based on Multi-Normal distribution\n",
    "1. Implement a Discrete Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Normal Naive Bayes Classifier Vs Normal Full Bayes Classifier\n",
    "In the following section we are going to compare 2 models on a given dataset. <br>\n",
    "The 2 classifiers we are going to test are:\n",
    "1. Naive Bayes classifer.<br>\n",
    "1. Full Bayes classifier.<br>\n",
    "Recall that a Naive Bayes classifier makes the following assumption :<br> \n",
    "## $$ p(x_1, x_2, ..., x_n|A_j) = \\Pi p(x_i | A_j) $$\n",
    "But the full Bayes classifier will not make this assumption.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Data Story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a faraway land called **Randomistan** there is a rare animal called the **Randomammal**.<br> \n",
    "We have gathered data about this unique animal to help the **randomian** researchers in observing this beast. <br>\n",
    "For a 1000 days straight we have measured the temperature and the humidity in Randomistan and whether the Randomammal was spotted or not. <br>\n",
    "The well known randomian **Bob** is a bit of a lazy researcher so he likes to keep things simple, and so he assumes that the temperature and the humidity are independent given the class. <br>\n",
    "**Alice** on the other hand is a hard working researcher and does not make any assumptions, she's young and is trying to gain some fame in the randomian community.\n",
    "\n",
    "The dataset contains 2 features (**Temperature**, **Humidity**) alongside a binary label (**Spotted**) for each instance.<br>\n",
    "\n",
    "We are going to test 2 different classifiers :\n",
    "* Naive Bayes Classifier (Bob)\n",
    "* Full Bayes Classifier. (Alice)\n",
    "\n",
    "Both of our researchers assume that our features are normally distributed. But while Bob with his Naive classifier will assume that the features are independent, Alice and her Full Bayes classifier will not make this assumption.<br><br>\n",
    "Let's start off by loading the data (train, test) into a pandas dataframe and then converting them\n",
    "into numpy arrays.<br>\n",
    "The datafiles are :\n",
    "- randomammal_train.csv\n",
    "- randomammal_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train and test set into a pandas dataframe and convert them into a numpy array.\n",
    "train_set = pd.read_csv('data/randomammal_train.csv').values\n",
    "test_set = pd.read_csv('data/randomammal_test.csv').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Visualization\n",
    "Draw a scatter plot of the training data where __x__=Temerature and **y**=Humidity. <br>\n",
    "Use color to distinguish points from different classes.<br>\n",
    "Stop for a minute to think about Alice and Bob's approaches and which one you expect to work better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bob's Naive Model\n",
    "\n",
    "Start with implementing the [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution) probability density function in the next cell: \n",
    "$$ \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\cdot e ^{-\\frac{(x - \\mu)^2}{2\\sigma^2}} $$\n",
    "Where :\n",
    "* $\\mu$ is the distribution mean.\n",
    "* $\\sigma$ is the distribution standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that when using the naive assumption, we assume our features are indepenent given the class. Meaning:\n",
    "$$ P(x_1, x_2 | Y) = p(x_1 | Y) \\cdot p(x_2 | Y)$$\n",
    "\n",
    "\n",
    "Since we assume our features are normally distributed we need to find the mean and std for each feature in order for us to compute those probabilites. Implement the **NaiveNormalClassDistribution** in the next cell and build a distribution object for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_pdf(x, mean, std):\n",
    "    \"\"\"\n",
    "    Calculate normal desnity function for a given x, mean and standrad deviation.\n",
    " \n",
    "    Input:\n",
    "    - x: A value we want to compute the distribution for.\n",
    "    - mean: The mean value of the distribution.\n",
    "    - std:  The standard deviation of the distribution.\n",
    " \n",
    "    Returns the normal distribution pdf according to the given mean and var for the given x.    \n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "class NaiveNormalClassDistribution():\n",
    "    def __init__(self, dataset, class_value):\n",
    "        \"\"\"\n",
    "        A class which encapsulates the relevant parameters(mean, std) for a class conditinoal normal distribution.\n",
    "        The mean and std are computed from a given data set.\n",
    "        \n",
    "        Input\n",
    "        - dataset: The dataset as a numpy array\n",
    "        - class_value : The class to calculate the parameters for.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_prior(self):\n",
    "        \"\"\"\n",
    "        Returns the prior porbability of the class according to the dataset distribution.\n",
    "        \"\"\"\n",
    "        return 1\n",
    "    \n",
    "    def get_instance_likelihood(self, x):\n",
    "        \"\"\"\n",
    "        Returns the likelihhod porbability of the instance under the class according to the dataset distribution.\n",
    "        \"\"\"\n",
    "        return 1\n",
    "    \n",
    "    def get_instance_posterior(self, x):\n",
    "        \"\"\"\n",
    "        Returns the posterior porbability of the instance under the class according to the dataset distribution.\n",
    "        * Ignoring p(x)\n",
    "        \"\"\"\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the a NaiveNormalClassDistribution for each class.\n",
    "naive_normal_CD_0 = NaiveNormalClassDistribution(train_set, 0)\n",
    "naive_normal_CD_1 = NaiveNormalClassDistribution(train_set, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the **MAPClassifier** class and build a MAPClassifier object containing the 2 distribution objects you just made above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAPClassifier():\n",
    "    def __init__(self, ccd0 , ccd1):\n",
    "        \"\"\"\n",
    "        A Maximum a posteriori classifier. \n",
    "        This class will hold 2 class distributions, one for class 0 and one for class 1, and will predict an instance\n",
    "        by the class that outputs the highest posterior probability for the given instance.\n",
    "    \n",
    "        Input\n",
    "            - ccd0 : An object contating the relevant parameters and methods for the distribution of class 0.\n",
    "            - ccd1 : An object contating the relevant parameters and methods for the distribution of class 1.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predicts the instance class using the 2 distribution objects given in the object constructor.\n",
    "    \n",
    "        Input\n",
    "            - An instance to predict.\n",
    "        Output\n",
    "            - 0 if the posterior probability of class 0 is higher and 1 otherwise.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_normal_classifier = MAPClassifier(naive_normal_CD_0, naive_normal_CD_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model\n",
    "Implement the **compute_accuracy** function in the next cell. Use it and the 2 distribution objects you created to compute the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(testset, map_classifier):\n",
    "    \"\"\"\n",
    "    Compute the accuracy of a given a testset using a MAP classifier object.\n",
    "    \n",
    "    Input\n",
    "        - testset: The testset for which to compute the accuracy (Numpy array).\n",
    "        - map_classifier : A MAPClassifier object capable of prediciting the class for each instance in the testset.\n",
    "        \n",
    "    Ouput\n",
    "        - Accuracy = #Correctly Classified / #testset size\n",
    "    \"\"\"\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the naive model accuracy and store it in the naive accuracy variable.\n",
    "naive_accuracy = compute_accuracy(test_set, naive_normal_classifier)\n",
    "naive_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alice's Full Model\n",
    "\n",
    "Start with Implementing the [multivariate normal](https://en.wikipedia.org/wiki/Multivariate_normal_distribution) distribution probability density function in the next cell.\n",
    "\n",
    "## $$ (2\\pi)^{-\\frac{d}{2}} det(\\Sigma )^{-\\frac{1}{2}} \\cdot e ^{-\\frac{1}{2}(x-\\mu)^T \\Sigma ^ {-1} (x - \\mu) }$$\n",
    "\n",
    "Where : \n",
    "* $\\mu$ is the distribution mean vector. (length 2 in our case)\n",
    "* $\\Sigma$ Is the distribution covarince matrix. (size 2x2 in our case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the full bayes model we will not make any simplyfing assumptions, meaning, we will use a multivariate normal distribution. <br>\n",
    "And so, we'll need to compute the mean of each feature and to compute the covariance between the features to build the covariance matrix.\n",
    "Implement the **MultiNormalClassDistribution** and build a distribution object for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_normal_pdf(x, mean, cov):\n",
    "    \"\"\"\n",
    "    Calculate multi variable normal desnity function for a given x, mean and covarince matrix.\n",
    " \n",
    "    Input:\n",
    "    - x: A value we want to compute the distribution for.\n",
    "    - mean: The mean value of the distribution.\n",
    "    - std:  The standard deviation of the distribution.\n",
    " \n",
    "    Returns the normal distribution pdf according to the given mean and var for the given x.    \n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "class MultiNormalClassDistribution():\n",
    "    def __init__(self, dataset, class_value):\n",
    "        \"\"\"\n",
    "        A class which encapsulate the relevant parameters(mean, cov matrix) for a class conditinoal multi normal distribution.\n",
    "        The mean and cov matrix (You can use np.cov for this!) will be computed from a given data set.\n",
    "        \n",
    "        Input\n",
    "        - dataset: The dataset as a numpy array\n",
    "        - class_value : The class to calculate the parameters for.\n",
    "        \"\"\"\n",
    "        \n",
    "    def get_prior(self):\n",
    "        \"\"\"\n",
    "        Returns the prior porbability of the class according to the dataset distribution.\n",
    "        \"\"\"\n",
    "        return 1\n",
    "    \n",
    "    def get_instance_likelihood(self, x):\n",
    "        \"\"\"\n",
    "        Returns the likelihood of the instance under the class according to the dataset distribution.\n",
    "        \"\"\"\n",
    "        return 1\n",
    "    \n",
    "    def get_instance_posterior(self, x):\n",
    "        \"\"\"\n",
    "        Returns the posterior porbability of the instance under the class according to the dataset distribution.\n",
    "        * Ignoring p(x)\n",
    "        \"\"\"\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the a MultiNormalClassDistribution for each class.\n",
    "multi_normal_CD_0 = MultiNormalClassDistribution(train_set, 0)\n",
    "multi_normal_CD_1 = MultiNormalClassDistribution(train_set, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build a MAPClassifier object contating the 2 distribution objects you just made above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_normal_classifier = MAPClassifier(multi_normal_CD_0, multi_normal_CD_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model\n",
    "Use the **compute_accuracy** function and the 2 distribution objects you created to compute the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the naive model accuracy and store it in the naive accuracy variable.\n",
    "full_accuracy = compute_accuracy(test_set, multi_normal_classifier)\n",
    "full_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a plot bar to showcase the models accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot of accuracy of each model side by side.\n",
    "plt.bar(x=['Naive', 'Full'], height=[naive_accuracy, full_accuracy])\n",
    "plt.title(\"Naive vs Full accuracy comparison\")\n",
    "plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Max a posteriori, prior, and likelihood results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the classifiers above (naive Bayes and full Bayes, in which we compare posterior probabilities), we explore how classifiers would perform if we compare (1) only prior probabilities or (2) only likelihoods. \n",
    "\n",
    "In this section, you will implement MaxPrior and MaxLikelihood classifiers similarly to MAPClassifier, and then graph the performance of the three models (MAP, MaxPrior, and MaxLikelihood) for each of the examples of above (naive Bayes and full Bayes).\n",
    "\n",
    "For example, your graph can have accuracy as the y-axis, \"MaxPrior\", \"MaxLikelihood\", and \"MAP\" as the x-axis values, and at each x-value, there will be two bars - one for the naive Bayes, and one for the full Bayes.  \n",
    "\n",
    "Other graphs (that make sense / are intuitive) will be accepted as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the **MaxPrior** class and build a MaxPrior object like you did above with the **MAPClassifier**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxPrior():\n",
    "    def __init__(self, ccd0 , ccd1):\n",
    "        \"\"\"\n",
    "        A Maximum prior classifier. \n",
    "        This class will hold 2 class distributions, one for class 0 and one for class 1, and will predicit an instance\n",
    "        by the class that outputs the highest prior probability for the given instance.\n",
    "    \n",
    "        Input\n",
    "            - ccd0 : An object contating the relevant parameters and methods for the distribution of class 0.\n",
    "            - ccd1 : An object contating the relevant parameters and methods for the distribution of class 1.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predicts the instance class using the 2 distribution objects given in the object constructor.\n",
    "    \n",
    "        Input\n",
    "            - An instance to predict.\n",
    "        Output\n",
    "            - 0 if the posterior probability of class 0 is higher and 1 otherwise.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the **MaxLikelihood** class and build a MaxLikelihood object like you did above with the **MAPClassifier**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxLikelihood():\n",
    "    def __init__(self, ccd0 , ccd1):\n",
    "        \"\"\"\n",
    "        A Maximum Likelihood classifier. \n",
    "        This class will hold 2 class distributions, one for class 0 and one for class 1, and will predicit an instance\n",
    "        by the class that outputs the highest likelihood probability for the given instance.\n",
    "    \n",
    "        Input\n",
    "            - ccd0 : An object contating the relevant parameters and methods for the distribution of class 0.\n",
    "            - ccd1 : An object contating the relevant parameters and methods for the distribution of class 1.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predicts the instance class using the 2 distribution objects given in the object constructor.\n",
    "    \n",
    "        Input\n",
    "            - An instance to predict.\n",
    "        Output\n",
    "            - 0 if the posterior probability of class 0 is higher and 1 otherwise.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run and evaluate the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the process you did for the MAPClassifier, now for the MaxPrior and MaxLikelihood classifiers:\n",
    "1. Feed the naive_normal distributions and the multi_normal distributions you made for each class into the new models you made in this section\n",
    "2. Evaluate the accuracies\n",
    "3. Plot the results as described in the beginning of this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "####### YOUR CODE HERE ########\n",
    "# you may add cells as needed #\n",
    "###############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now build a discrete naive Bayes based classifier using **Laplace** smoothing.\n",
    "In the recitation, we saw how to compute the probability for each attribute value under each class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ P(x_j | A_i) = \\frac{n_{ij} + 1}{n_i + |V_j|} $$\n",
    "Where:\n",
    "* $n_{ij}$ The number of training instances with the class $A_i$ and the value $x_j$ in the relevant attribute.\n",
    "* $n_i$ The number of training instances with the class $A_i$\n",
    "* $|V_j|$ The number of possible values of the relevant attribute.\n",
    "\n",
    "In order to compute the likelihood we assume:\n",
    "$$ P(x| A_i) = \\prod\\limits_{j=1}^{n}P(x_j|A_i) $$\n",
    "\n",
    "And to classify an instance we will choose : \n",
    "$$\\arg\\!\\max\\limits_{i} P(A_i) \\cdot P(x | A_i)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "We will try to predict breast cancer again only this time from a different dataset, \n",
    "<br> you can read about the dataset here : [Breast Cancer Dataset](https://archive.ics.uci.edu/ml/datasets/breast+cancer)<br>\n",
    "Load the training set and test set provided for you in the data folder.\n",
    " - breast_trainset.csv\n",
    " - breast_testset.csv\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the train and test set into a pandas dataframe and convert them into a numpy array.\n",
    "train_set = pd.read_csv('data/breast_trainset.csv').values\n",
    "test_set = pd.read_csv('data/breast_testset.csv').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build A Discrete Naive Bayes Distribution for each class\n",
    "Implement the **DiscreteNBClassDistribution** in the next cell and build a distribution object for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILLON = 1e-6 # if a certain value only occurs in the test set, the probability for that value will be EPSILLON.\n",
    "\n",
    "class DiscreteNBClassDistribution():\n",
    "    def __init__(self, dataset, class_value):\n",
    "        \"\"\"\n",
    "        A class which computes and encapsulate the relevant probabilites for a discrete naive bayes \n",
    "        distribution for a specific class. The probabilites are computed with laplace smoothing.\n",
    "        \n",
    "        Input\n",
    "        - dataset: The dataset as a numpy array.\n",
    "        - class_value: Compute the relevant parameters only for instances from the given class.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_prior(self):\n",
    "        \"\"\"\n",
    "        Returns the prior porbability of the class according to the dataset distribution.\n",
    "        \"\"\"\n",
    "        return 1\n",
    "    \n",
    "    def get_instance_likelihood(self, x):\n",
    "        \"\"\"\n",
    "        Returns the likelihood of the instance under the class according to the dataset distribution.\n",
    "        \"\"\"\n",
    "        return 1\n",
    "    \n",
    "    def get_instance_posterior(self, x):\n",
    "        \"\"\"\n",
    "        Returns the posterior porbability of the instance under the class according to the dataset distribution.\n",
    "        * Ignoring p(x)\n",
    "        \"\"\"\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_naive_CD_0 = DiscreteNBClassDistribution(train_set, 0)\n",
    "discrete_naive_CD_1 = DiscreteNBClassDistribution(train_set, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build a MAPClassifier object contating the 2 distribution objects you just made above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discrete_naive_classifier = MAPClassifier(discrete_naive_CD_0, discrete_naive_CD_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the **compute_accuracy** function and the 2 distribution objects you created to compute the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_accuracy(test_set, discrete_naive_classifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
