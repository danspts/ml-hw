{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sympy.interactive import printing\n",
    "printing.init_printing(use_latex='mathjax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-35def0d0f4b47a0a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Exercise 1: Linear Regression\n",
    "\n",
    "### This notebook is executed automatically. Failing to meet any of the submission requirements will results in a 25 point fine or your submission not being graded at all. Kindly reminder: the homework assignments grade is 50% of the final grade. \n",
    "\n",
    "### Do not start the exercise until you fully understand the submission guidelines.\n",
    "\n",
    "## Read the following instructions carefully:\n",
    "\n",
    "1. This jupyter notebook contains all the step by step instructions needed for this exercise.\n",
    "1. Submission includes this notebook only with the exercise number and your ID as the filename. For example: `hw1_123456789_987654321.ipynb` if you submitted in pairs and `hw1_123456789.ipynb` if you submitted the exercise alone.\n",
    "1. Write **efficient vectorized** code whenever possible. Some calculations in this exercise take several minutes when implemented efficiently, and might take much longer otherwise. Unnecessary loops will result in point deduction.\n",
    "1. You are responsible for the correctness of your code and should add as many tests as you see fit. Tests will not be graded nor checked.\n",
    "1. Write your functions in this notebook only. **Do not create Python modules and import them**.\n",
    "1. You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/) and [numpy](https://www.numpy.org/devdocs/reference/) only. **Do not import anything else.**\n",
    "1. Your code must run without errors. Make sure your `numpy` version is at least 1.15.4 and that you are using at least python 3.6. Changes of the configuration we provided are at your own risk. Any code that cannot run will not be graded.\n",
    "1. Write your own code. Cheating will not be tolerated.\n",
    "1. Answers to qualitative questions should be written in **markdown** cells (with $\\LaTeX$ support). Answers that will be written in commented code blocks will not be checked.\n",
    "\n",
    "## In this exercise you will perform the following:\n",
    "1. Load a dataset and perform basic data exploration using a powerful data science library called [pandas](https://pandas.pydata.org/pandas-docs/stable/).\n",
    "1. Preprocess the data for linear regression.\n",
    "1. Compute the cost and perform gradient descent in pure numpy in vectorized form.\n",
    "1. Fit a linear regression model using a single feature.\n",
    "1. Visualize your results using matplotlib.\n",
    "1. Perform multivariate linear regression.\n",
    "1. Pick the best features in the dataset.\n",
    "1. Experiment with adaptive learning rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I have read and understood the instructions: 323081950, 227367455"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5ed0076cec86f623",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # used for scientific computing\n",
    "import pandas as pd # used for data analysis and manipulation\n",
    "import matplotlib.pyplot as plt # used for visualization and plotting\n",
    "\n",
    "np.random.seed(42) \n",
    "\n",
    "# make matplotlib figures appear inline in the notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (14.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-916f46de8cde2ca7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Part 1: Data Preprocessing (10 Points)\n",
    "\n",
    "For the following exercise, we will use a dataset containing housing prices in King County, USA. The dataset contains 5,000 observations with 18 features and a single target value - the house price. \n",
    "\n",
    "First, we will read and explore the data using pandas and the `.read_csv` method. Pandas is an open source library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9ef8b2769c2c1949",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Read comma separated data\n",
    "df = pd.read_csv('data.csv') # Make sure this cell runs regardless of your absolute path.\n",
    "# df stands for dataframe, which is the default format for datasets in pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6966afc155aa6616",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Data Exploration\n",
    "A good practice in any data-oriented project is to first try and understand the data. Fortunately, pandas is built for that purpose. Start by looking at the top of the dataset using the `df.head()` command. This will be the first indication that you read your data properly, and that the headers are correct. Next, you can use `df.describe()` to show statistics on the data and check for trends and irregularities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date     price  bedrooms  bathrooms  sqft_living  \\\n",
       "0  7129300520  20141013T000000  221900.0         3       1.00         1180   \n",
       "1  6414100192  20141209T000000  538000.0         3       2.25         2570   \n",
       "2  5631500400  20150225T000000  180000.0         2       1.00          770   \n",
       "3  2487200875  20141209T000000  604000.0         4       3.00         1960   \n",
       "4  1954400510  20150218T000000  510000.0         3       2.00         1680   \n",
       "\n",
       "   sqft_lot  floors  waterfront  view  condition  grade  sqft_above  yr_built  \\\n",
       "0      5650     1.0           0     0          3      7        1180      1955   \n",
       "1      7242     2.0           0     0          3      7        2170      1951   \n",
       "2     10000     1.0           0     0          3      6         770      1933   \n",
       "3      5000     1.0           0     0          5      7        1050      1965   \n",
       "4      8080     1.0           0     0          3      8        1680      1987   \n",
       "\n",
       "   yr_renovated  zipcode      lat     long  sqft_living15  sqft_lot15  \n",
       "0             0    98178  47.5112 -122.257           1340        5650  \n",
       "1          1991    98125  47.7210 -122.319           1690        7639  \n",
       "2             0    98028  47.7379 -122.233           2720        8062  \n",
       "3             0    98136  47.5208 -122.393           1360        5000  \n",
       "4             0    98074  47.6168 -122.045           1800        7503  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-5bd0d6844b64ea1a",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000e+03</td>\n",
       "      <td>5.000000e+03</td>\n",
       "      <td>5000.0000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5.000000e+03</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.000000</td>\n",
       "      <td>5000.00000</td>\n",
       "      <td>5000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.630823e+09</td>\n",
       "      <td>5.394699e+05</td>\n",
       "      <td>3.3714</td>\n",
       "      <td>2.062150</td>\n",
       "      <td>2061.036800</td>\n",
       "      <td>1.615893e+04</td>\n",
       "      <td>1.432600</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>0.243000</td>\n",
       "      <td>3.455000</td>\n",
       "      <td>7.595200</td>\n",
       "      <td>1753.151000</td>\n",
       "      <td>1966.660800</td>\n",
       "      <td>95.052800</td>\n",
       "      <td>98078.812600</td>\n",
       "      <td>47.559312</td>\n",
       "      <td>-122.215864</td>\n",
       "      <td>1976.84520</td>\n",
       "      <td>13451.164600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.870890e+09</td>\n",
       "      <td>3.873115e+05</td>\n",
       "      <td>0.9104</td>\n",
       "      <td>0.773592</td>\n",
       "      <td>923.727509</td>\n",
       "      <td>4.600220e+04</td>\n",
       "      <td>0.510793</td>\n",
       "      <td>0.096506</td>\n",
       "      <td>0.774643</td>\n",
       "      <td>0.677692</td>\n",
       "      <td>1.166537</td>\n",
       "      <td>818.390844</td>\n",
       "      <td>28.286855</td>\n",
       "      <td>425.234932</td>\n",
       "      <td>54.126332</td>\n",
       "      <td>0.139521</td>\n",
       "      <td>0.141807</td>\n",
       "      <td>674.73601</td>\n",
       "      <td>26514.749009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000102e+06</td>\n",
       "      <td>7.500000e+04</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>380.000000</td>\n",
       "      <td>6.090000e+02</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>380.000000</td>\n",
       "      <td>1900.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98001.000000</td>\n",
       "      <td>47.155900</td>\n",
       "      <td>-122.514000</td>\n",
       "      <td>620.00000</td>\n",
       "      <td>660.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.154075e+09</td>\n",
       "      <td>3.179062e+05</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1410.000000</td>\n",
       "      <td>5.400000e+03</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1190.000000</td>\n",
       "      <td>1949.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98033.000000</td>\n",
       "      <td>47.463675</td>\n",
       "      <td>-122.329000</td>\n",
       "      <td>1490.00000</td>\n",
       "      <td>5391.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.022900e+09</td>\n",
       "      <td>4.490000e+05</td>\n",
       "      <td>3.0000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1890.000000</td>\n",
       "      <td>7.875000e+03</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>1530.000000</td>\n",
       "      <td>1968.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98070.000000</td>\n",
       "      <td>47.572850</td>\n",
       "      <td>-122.235000</td>\n",
       "      <td>1820.00000</td>\n",
       "      <td>7800.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.345078e+09</td>\n",
       "      <td>6.500000e+05</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2500.000000</td>\n",
       "      <td>1.123400e+04</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2130.000000</td>\n",
       "      <td>1990.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98118.000000</td>\n",
       "      <td>47.679200</td>\n",
       "      <td>-122.129000</td>\n",
       "      <td>2340.00000</td>\n",
       "      <td>10469.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.842300e+09</td>\n",
       "      <td>7.060000e+06</td>\n",
       "      <td>9.0000</td>\n",
       "      <td>6.750000</td>\n",
       "      <td>10040.000000</td>\n",
       "      <td>1.651359e+06</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>7680.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "      <td>98199.000000</td>\n",
       "      <td>47.777600</td>\n",
       "      <td>-121.315000</td>\n",
       "      <td>5790.00000</td>\n",
       "      <td>434728.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id         price   bedrooms    bathrooms   sqft_living  \\\n",
       "count  5.000000e+03  5.000000e+03  5000.0000  5000.000000   5000.000000   \n",
       "mean   4.630823e+09  5.394699e+05     3.3714     2.062150   2061.036800   \n",
       "std    2.870890e+09  3.873115e+05     0.9104     0.773592    923.727509   \n",
       "min    1.000102e+06  7.500000e+04     0.0000     0.000000    380.000000   \n",
       "25%    2.154075e+09  3.179062e+05     3.0000     1.500000   1410.000000   \n",
       "50%    4.022900e+09  4.490000e+05     3.0000     2.000000   1890.000000   \n",
       "75%    7.345078e+09  6.500000e+05     4.0000     2.500000   2500.000000   \n",
       "max    9.842300e+09  7.060000e+06     9.0000     6.750000  10040.000000   \n",
       "\n",
       "           sqft_lot       floors   waterfront         view    condition  \\\n",
       "count  5.000000e+03  5000.000000  5000.000000  5000.000000  5000.000000   \n",
       "mean   1.615893e+04     1.432600     0.009400     0.243000     3.455000   \n",
       "std    4.600220e+04     0.510793     0.096506     0.774643     0.677692   \n",
       "min    6.090000e+02     1.000000     0.000000     0.000000     1.000000   \n",
       "25%    5.400000e+03     1.000000     0.000000     0.000000     3.000000   \n",
       "50%    7.875000e+03     1.000000     0.000000     0.000000     3.000000   \n",
       "75%    1.123400e+04     2.000000     0.000000     0.000000     4.000000   \n",
       "max    1.651359e+06     3.500000     1.000000     4.000000     5.000000   \n",
       "\n",
       "             grade   sqft_above     yr_built  yr_renovated       zipcode  \\\n",
       "count  5000.000000  5000.000000  5000.000000   5000.000000   5000.000000   \n",
       "mean      7.595200  1753.151000  1966.660800     95.052800  98078.812600   \n",
       "std       1.166537   818.390844    28.286855    425.234932     54.126332   \n",
       "min       3.000000   380.000000  1900.000000      0.000000  98001.000000   \n",
       "25%       7.000000  1190.000000  1949.000000      0.000000  98033.000000   \n",
       "50%       7.000000  1530.000000  1968.000000      0.000000  98070.000000   \n",
       "75%       8.000000  2130.000000  1990.000000      0.000000  98118.000000   \n",
       "max      13.000000  7680.000000  2015.000000   2015.000000  98199.000000   \n",
       "\n",
       "               lat         long  sqft_living15     sqft_lot15  \n",
       "count  5000.000000  5000.000000     5000.00000    5000.000000  \n",
       "mean     47.559312  -122.215864     1976.84520   13451.164600  \n",
       "std       0.139521     0.141807      674.73601   26514.749009  \n",
       "min      47.155900  -122.514000      620.00000     660.000000  \n",
       "25%      47.463675  -122.329000     1490.00000    5391.500000  \n",
       "50%      47.572850  -122.235000     1820.00000    7800.000000  \n",
       "75%      47.679200  -122.129000     2340.00000   10469.250000  \n",
       "max      47.777600  -121.315000     5790.00000  434728.000000  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9b9bd1b387905904",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We will start with one variable linear regression by extracting the target column and the `sqft_living` variable from the dataset. We use pandas and select both columns as separate variables and transform them into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c7cd243e8b5fe5aa",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "X = df['sqft_living'].values\n",
    "y = df['price'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-508e7e1a13f9bbe4",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "As the number of features grows, calculating gradients gets computationally expensive. We can speed this up by normalizing the input data to ensure all values are within the same range. This is especially important for datasets with high standard deviations or differences in the ranges of the attributes. Use [mean normalization](https://en.wikipedia.org/wiki/Feature_scaling) for the fearures (`X`) and the true labels (`y`).\n",
    "\n",
    "Implement the cost function `preprocess`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(X, y):\n",
    "    \"\"\"\n",
    "    Perform mean normalization on the features and true labels.\n",
    "\n",
    "    Input:\n",
    "    - X: Inputs (n features over m instances).\n",
    "    - y: True labels.\n",
    "\n",
    "    Returns a two vales:\n",
    "    - X: The mean normalized inputs.\n",
    "    - y: The mean normalized labels.\n",
    "    \"\"\"\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the normalization function.                             #\n",
    "    ###########################################################################\n",
    "    x_norm = X / X.max(axis=0)\n",
    "    y_norm = y / y.max(axis=0)\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return x_norm, y_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-9bb6a28b6b6932fa",
     "locked": false,
     "points": 5,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "X, y = preprocess(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the data into two datasets: \n",
    "1. The training dataset will contain 80% of the data and will always be used for model training.\n",
    "2. The validation dataset will contain the remaining 20% of the data and will be used for model evaluation. For example, we will pick the best alpha and the best features using the validation dataset, while still training the model using the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation split\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(X.shape[0])\n",
    "idx_train, idx_val = indices[:int(0.8*X.shape[0])], indices[int(0.8*X.shape[0]):]\n",
    "X_train, X_val = X[idx_train], X[idx_val]\n",
    "y_train, y_val = y[idx_train], y[idx_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0c168d036748663e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Data Visualization\n",
    "Another useful tool is data visualization. Since this problem has only two parameters, it is possible to create a two-dimensional scatter plot to visualize the data. Note that many real-world datasets are highly dimensional and cannot be visualized naively. We will be using `matplotlib` for all data visualization purposes since it offers a wide range of visualization tools and is easy to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cbad8871e083093f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X_train, y_train, 'ro', ms=1, mec='k') # the parameters control the size, shape and color of the scatter plot\n",
    "plt.ylabel('Price in USD')\n",
    "plt.xlabel('sq.ft')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c50f0a0e569142ed",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Bias Trick\n",
    "\n",
    "Make sure that `X` takes into consideration the bias $\\theta_0$ in the linear model. Hint, recall that the predications of our linear model are of the form:\n",
    "$\n",
    "\\hat{y} = h_\\theta(x) = \\theta^T x = \\theta_0 + \\theta_1 x_1\n",
    "$\n",
    "\n",
    "Add columns of ones as the zeroth column of the features (do this for both the training and validation sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-44853962dc1651df",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "#                            START OF YOUR CODE                           #\n",
    "###########################################################################\n",
    "X_train = np.transpose(np.vstack((np.ones(X_train.shape[0], dtype=X_train.dtype), X_train)))\n",
    "X_val = np.transpose(np.vstack((np.ones(X_val.shape[0], dtype=X_train.dtype), X_val)))\n",
    "###########################################################################\n",
    "#                             END OF YOUR CODE                            #\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c7d7fd68c1b24943",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Part 2: Single Variable Linear Regression (40 Points)\n",
    "Simple linear regression is a linear regression model with a single explanatory varaible and a single target value. \n",
    "\n",
    "$\n",
    "\\hat{y} = h_\\theta(x) = \\theta^T x = \\theta_0 + \\theta_1 x_1\n",
    "$\n",
    "\n",
    "## Gradient Descent \n",
    "\n",
    "Our task is to find the best possible linear line that explains all the points in our dataset. We start by guessing initial values for the linear regression parameters $\\theta$ and updating the values using gradient descent. \n",
    "\n",
    "The objective of linear regression is to minimize the cost function $J$:\n",
    "\n",
    "$\n",
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{n}(h_\\theta(x^{(i)})-y^{(i)})^2\n",
    "$\n",
    "\n",
    "where the hypothesis (model) $h_\\theta(x)$ is given by a **linear** model:\n",
    "\n",
    "$\n",
    "h_\\theta(x) = \\theta^T x = \\theta_0 + \\theta_1 x_1\n",
    "$\n",
    "\n",
    "$\\theta_j$ are parameters of your model. and by changing those values accordingly you will be able to lower the cost function $J(\\theta)$. One way to accopmlish this is to use gradient descent:\n",
    "\n",
    "$\n",
    "\\theta_j = \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)})-y^{(i)})x_j^{(i)}\n",
    "$\n",
    "\n",
    "In linear regresion, we know that with each step of gradient descent, the parameters $\\theta_j$ get closer to the optimal values that will achieve the lowest cost $J(\\theta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0f83af93c0436542",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Implement the cost function `compute_cost`. (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, theta):\n",
    "    \"\"\"\n",
    "    Computes the average squared difference between an obserbation's actual and\n",
    "    predicted values for linear regression.  \n",
    "\n",
    "    Input:\n",
    "    - X: inputs  (n features over m instances).\n",
    "    - y: true labels (1 value over m instances).\n",
    "    - theta: the parameters (weights) of the model being learned.\n",
    "\n",
    "    Returns a single value:\n",
    "    - J: the cost associated with the current set of parameters (single number).\n",
    "    \"\"\"\n",
    "\n",
    "    J = 0  # Use J for the cost.\n",
    "\n",
    "#     print(X.shape)\n",
    "#     for i in range(X.shape[0] - 1):\n",
    "#         temp = (np.transpose(theta) * X[i])\n",
    "#         J += (temp - y[i - 1])**2 \n",
    "#     J /= 2*X.shape[1]\n",
    "        \n",
    "    ###########################################################################\n",
    "    # TODO: Implement the MSE cost function.                                  #\n",
    "    ###########################################################################\n",
    "    J = sum([(np.dot(theta, X[i]) - y[i])**2 for i in range(1,X.shape[0])]) / (2*X.shape[0])\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4c1cfec24e144479",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "theta = np.array([-1, 2])\n",
    "J = compute_cost(X_train, y_train, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 0.232144150122662$"
      ],
      "text/plain": [
       "0.23214415012266243"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the gradient descent function `gradient_descent`. (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-afdc527b73d275bb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "np.random.seed(42)\n",
    "theta = np.random.random(size=2)\n",
    "iterations = 40000\n",
    "alpha = 0.1\n",
    "theta, J_history = gradient_descent(X_train ,y_train, theta, alpha, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Learn the parameters of the model using gradient descent using \n",
    "    the *training set*. Gradient descent is an optimization algorithm \n",
    "    used to minimize some (loss) function by iteratively moving in \n",
    "    the direction of steepest descent as defined by the negative of \n",
    "    the gradient. We use gradient descent to update the parameters\n",
    "    (weights) of our model.\n",
    "\n",
    "    Input:\n",
    "    - X: Inputs  (n features over m instances).\n",
    "    - y: True labels (1 value over m instances).\n",
    "    - theta: The parameters (weights) of the model being learned.\n",
    "    - alpha: The learning rate of your model.\n",
    "    - num_iters: The number of updates performed.\n",
    "\n",
    "    Returns two values:\n",
    "    - theta: The learned parameters of your model.\n",
    "    - J_history: the loss value for every iteration.\n",
    "    \"\"\"\n",
    "    \n",
    "    J_history = [] # Use a python list to save cost in every iteration\n",
    "    theta = theta.copy() # avoid changing the original thetas\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the gradient descent optimization algorithm.            #\n",
    "    ###########################################################################\n",
    "    for n in range(num_iters):\n",
    "        theta -= alpha * sum([(np.dot(theta, X[i]) - y[i]) * X[i] for i in range(0,X.shape[0])]) / X.shape[0]\n",
    "        J_history.append(compute_cost(X, y, theta))\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left( 2,\\right)$"
      ],
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-59b95cbea13e7fc1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "theta = np.random.random(size=2)\n",
    "iterations = 400\n",
    "alpha = 0.1\n",
    "theta, J_history = gradient_descent(X_train ,y_train, theta, alpha, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left[ 0.0996413896473205, \\  0.0802539347725695, \\  0.0646992225906663, \\  0.0522194990639092, \\  0.0422068224785505, \\  0.0341734446717379, \\  0.0277280480649292, \\  0.0225566807760504, \\  0.0184074609723385, \\  0.0150783052626262, \\  0.0124070832585452, \\  0.0102637186366886, \\  0.00854385186697694, \\  0.00716375585659531, \\  0.0060562568006983, \\  0.00516746150458528, \\  0.00445413173319118, \\  0.00388157766679542, \\  0.00342196783260747, \\  0.00305297317251473, \\  0.00275667918632528, \\  0.00251871315042179, \\  0.002327543890172, \\  0.00217391999122142, \\  0.0020504190795057, \\  0.00195108621106096, \\  0.00187114375411727, \\  0.00180675862904655, \\  0.001754855566194, \\  0.00171296728360062, \\  0.00167911428534956, \\  0.00165170842437658, \\  0.0016294755313806, \\  0.00161139334036407, \\  0.00159664168657819, \\  0.00158456255055456, \\  0.00157462800160094, \\  0.00156641447899771, \\  0.00155958215789995, \\  0.00155385839467434, \\  0.00154902444514766, \\  0.00154490480869723, \\  0.0015413586790429, \\  0.00153827308523691, \\  0.00153555738869267, \\  0.00153313886815851, \\  0.00153095917754583, \\  0.00152897150404554, \\  0.00152713828808378, \\  0.00152542939403982, \\  0.00152382064260943, \\  0.00152229263331586, \\  0.00152082979980591, \\  0.00151941965190921, \\  0.00151805216753763, \\  0.0015167193048016, \\  0.00151541461057653, \\  0.00151413290645132, \\  0.0015128700367609, \\  0.00151162266642885, \\  0.00151038811877315, \\  0.00150916424537428, \\  0.00150794932166761, \\  0.00150674196317427, \\  0.00150554105829067, \\  0.00150434571436332, \\  0.00150315521442245, \\  0.00150196898246757, \\  0.00150078655561432, \\  0.00149960756174632, \\  0.00149843170158385, \\  0.0014972587342963, \\  0.00149608846595782, \\  0.00149492074028433, \\  0.00149375543120085, \\  0.00149259243687745, \\  0.00149143167494348, \\  0.00149027307864738, \\  0.00148911659377497, \\  0.00148796217617651, \\  0.00148680978978221, \\  0.00148565940500948, \\  0.00148451099748488, \\  0.00148336454701821, \\  0.00148222003677922, \\  0.00148107745263672, \\  0.00147993678262814, \\  0.00147879801653375, \\  0.00147766114553477, \\  0.00147652616193897, \\  0.00147539305896034, \\  0.00147426183054209, \\  0.00147313247121465, \\  0.00147200497598151, \\  0.00147087934022757, \\  0.00146975555964552, \\  0.00146863363017667, \\  0.00146751354796341, \\  0.00146639530931101, \\  0.00146527891065688, \\  0.00146416434854592, \\  0.00146305161961058, \\  0.00146194072055494, \\  0.00146083164814182, \\  0.00145972439918244, \\  0.00145861897052803, \\  0.00145751535906322, \\  0.00145641356170048, \\  0.00145531357537586, \\  0.00145421539704541, \\  0.00145311902368235, \\  0.00145202445227476, \\  0.00145093167982373, \\  0.00144984070334181, \\  0.00144875151985184, \\  0.00144766412638594, \\  0.00144657851998467, \\  0.00144549469769642, \\  0.00144441265657683, \\  0.00144333239368842, \\  0.00144225390610013, \\  0.00144117719088713, \\  0.00144010224513049, \\  0.00143902906591706, \\  0.00143795765033923, \\  0.00143688799549484, \\  0.00143582009848707, \\  0.00143475395642428, \\  0.00143368956642002, \\  0.00143262692559289, \\  0.00143156603106652, \\  0.0014305068799695, \\  0.00142944946943534, \\  0.00142839379660244, \\  0.00142733985861408, \\  0.00142628765261829, \\  0.00142523717576798, \\  0.00142418842522081, \\  0.00142314139813917, \\  0.0014220960916902, \\  0.00142105250304578, \\  0.00142001062938246, \\  0.00141897046788151, \\  0.00141793201572885, \\  0.00141689527011507, \\  0.00141586022823542, \\  0.00141482688728978, \\  0.00141379524448265, \\  0.00141276529702316, \\  0.00141173704212504, \\  0.00141071047700661, \\  0.00140968559889078, \\  0.00140866240500506, \\  0.0014076408925815, \\  0.00140662105885672, \\  0.00140560290107188, \\  0.00140458641647271, \\  0.00140357160230945, \\  0.00140255845583688, \\  0.00140154697431427, \\  0.00140053715500545, \\  0.00139952899517869, \\  0.00139852249210681, \\  0.00139751764306709, \\  0.00139651444534127, \\  0.00139551289621559, \\  0.00139451299298075, \\  0.00139351473293188, \\  0.00139251811336858, \\  0.00139152313159489, \\  0.00139052978491927, \\  0.00138953807065462, \\  0.00138854798611822, \\  0.00138755952863182, \\  0.00138657269552153, \\  0.00138558748411786, \\  0.00138460389175572, \\  0.0013836219157744, \\  0.00138264155351753, \\  0.00138166280233317, \\  0.00138068565957368, \\  0.00137971012259581, \\  0.00137873618876062, \\  0.00137776385543353, \\  0.0013767931199843, \\  0.00137582397978699, \\  0.00137485643221998, \\  0.00137389047466596, \\  0.00137292610451195, \\  0.00137196331914922, \\  0.00137100211597334, \\  0.00137004249238417, \\  0.00136908444578585, \\  0.00136812797358677, \\  0.00136717307319958, \\  0.0013662197420412, \\  0.00136526797753278, \\  0.0013643177770997, \\  0.00136336913817159, \\  0.00136242205818229, \\  0.00136147653456988, \\  0.00136053256477662, \\  0.00135959014624898, \\  0.00135864927643766, \\  0.00135770995279749, \\  0.00135677217278754, \\  0.00135583593387103, \\  0.00135490123351535, \\  0.00135396806919204, \\  0.00135303643837682, \\  0.00135210633854955, \\  0.00135117776719421, \\  0.00135025072179896, \\  0.00134932519985606, \\  0.00134840119886186, \\  0.00134747871631688, \\  0.00134655774972573, \\  0.00134563829659711, \\  0.00134472035444381, \\  0.00134380392078273, \\  0.00134288899313484, \\  0.00134197556902517, \\  0.00134106364598283, \\  0.00134015322154101, \\  0.00133924429323691, \\  0.00133833685861182, \\  0.00133743091521104, \\  0.00133652646058393, \\  0.00133562349228385, \\  0.00133472200786819, \\  0.00133382200489838, \\  0.00133292348093982, \\  0.00133202643356193, \\  0.00133113086033812, \\  0.00133023675884581, \\  0.00132934412666635, \\  0.00132845296138512, \\  0.00132756326059143, \\  0.00132667502187857, \\  0.00132578824284378, \\  0.00132490292108825, \\  0.00132401905421712, \\  0.00132313663983944, \\  0.00132225567556822, \\  0.00132137615902039, \\  0.00132049808781676, \\  0.00131962145958209, \\  0.00131874627194504, \\  0.00131787252253813, \\  0.00131700020899782, \\  0.00131612932896442, \\  0.00131525988008215, \\  0.00131439185999905, \\  0.00131352526636707, \\  0.001312660096842, \\  0.00131179634908351, \\  0.00131093402075506, \\  0.00131007310952401, \\  0.00130921361306152, \\  0.00130835552904257, \\  0.001307498855146, \\  0.00130664358905442, \\  0.00130578972845428, \\  0.0013049372710358, \\  0.00130408621449304, \\  0.00130323655652382, \\  0.00130238829482974, \\  0.0013015414271162, \\  0.00130069595109234, \\  0.00129985186447109, \\  0.00129900916496913, \\  0.00129816785030691, \\  0.00129732791820859, \\  0.00129648936640209, \\  0.00129565219261907, \\  0.0012948163945949, \\  0.0012939819700687, \\  0.00129314891678328, \\  0.00129231723248516, \\  0.00129148691492458, \\  0.00129065796185548, \\  0.00128983037103546, \\  0.00128900414022583, \\  0.00128817926719157, \\  0.00128735574970136, \\  0.0012865335855275, \\  0.00128571277244597, \\  0.00128489330823644, \\  0.00128407519068218, \\  0.00128325841757011, \\  0.00128244298669082, \\  0.0012816288958385, \\  0.00128081614281098, \\  0.00128000472540969, \\  0.00127919464143971, \\  0.00127838588870968, \\  0.00127757846503188, \\  0.00127677236822217, \\  0.0012759675961, \\  0.0012751641464884, \\  0.00127436201721398, \\  0.00127356120610693, \\  0.00127276171100099, \\  0.00127196352973348, \\  0.00127116666014524, \\  0.00127037110008072, \\  0.00126957684738784, \\  0.0012687838999181, \\  0.00126799225552654, \\  0.00126720191207168, \\  0.0012664128674156, \\  0.00126562511942387, \\  0.00126483866596559, \\  0.00126405350491334, \\  0.00126326963414322, \\  0.00126248705153478, \\  0.00126170575497109, \\  0.0012609257423387, \\  0.00126014701152762, \\  0.00125936956043132, \\  0.00125859338694676, \\  0.00125781848897431, \\  0.00125704486441784, \\  0.00125627251118465, \\  0.00125550142718545, \\  0.00125473161033442, \\  0.00125396305854915, \\  0.00125319576975066, \\  0.00125242974186338, \\  0.00125166497281516, \\  0.00125090146053724, \\  0.00125013920296428, \\  0.00124937819803431, \\  0.00124861844368877, \\  0.00124785993787247, \\  0.00124710267853361, \\  0.00124634666362376, \\  0.00124559189109782, \\  0.00124483835891412, \\  0.00124408606503429, \\  0.00124333500742332, \\  0.00124258518404956, \\  0.00124183659288469, \\  0.00124108923190372, \\  0.00124034309908499, \\  0.00123959819241018, \\  0.00123885450986425, \\  0.00123811204943549, \\  0.00123737080911552, \\  0.00123663078689922, \\  0.00123589198078479, \\  0.00123515438877373, \\  0.00123441800887077, \\  0.00123368283908399, \\  0.0012329488774247, \\  0.00123221612190749, \\  0.0012314845705502, \\  0.00123075422137396, \\  0.00123002507240312, \\  0.00122929712166529, \\  0.00122857036719131, \\  0.00122784480701529, \\  0.00122712043917451, \\  0.00122639726170954, \\  0.00122567527266414, \\  0.00122495447008528, \\  0.00122423485202315, \\  0.00122351641653113, \\  0.00122279916166583, \\  0.001222083085487, \\  0.00122136818605765, \\  0.00122065446144392, \\  0.00121994190971514, \\  0.00121923052894382, \\  0.00121852031720564, \\  0.00121781127257943, \\  0.00121710339314718, \\  0.00121639667699404, \\  0.00121569112220831, \\  0.00121498672688141, \\  0.00121428348910792, \\  0.00121358140698554, \\  0.0012128804786151, \\  0.00121218070210056, \\  0.00121148207554897, \\  0.00121078459707051, \\  0.00121008826477847, \\  0.00120939307678922, \\  0.00120869903122225, \\  0.00120800612620012, \\  0.00120731435984849, \\  0.0012066237302961, \\  0.00120593423567474, \\  0.00120524587411931, \\  0.00120455864376776, \\  0.00120387254276107, \\  0.00120318756924332, \\  0.00120250372136162, \\  0.00120182099726611, \\  0.00120113939511\\right]$"
      ],
      "text/plain": [
       "[0.0996413896473205, 0.0802539347725695, 0.06469922259066635, 0.05221949906390\n",
       "925, 0.04220682247855051, 0.03417344467173795, 0.027728048064929237, 0.0225566\n",
       "80776050362, 0.018407460972338544, 0.015078305262626223, 0.012407083258545185,\n",
       " 0.010263718636688554, 0.008543851866976945, 0.007163755856595308, 0.006056256\n",
       "800698304, 0.005167461504585275, 0.004454131733191176, 0.003881577666795418, 0\n",
       ".003421967832607471, 0.0030529731725147265, 0.0027566791863252783, 0.002518713\n",
       "1504217867, 0.002327543890171997, 0.0021739199912214186, 0.0020504190795056997\n",
       ", 0.0019510862110609603, 0.0018711437541172658, 0.0018067586290465476, 0.00175\n",
       "48555661939967, 0.0017129672836006158, 0.0016791142853495636, 0.00165170842437\n",
       "6583, 0.0016294755313805967, 0.0016113933403640686, 0.0015966416865781947, 0.0\n",
       "015845625505545604, 0.0015746280016009355, 0.0015664144789977097, 0.0015595821\n",
       "578999456, 0.001553858394674339, 0.0015490244451476598, 0.001544904808697226, \n",
       "0.001541358679042895, 0.0015382730852369104, 0.001535557388692674, 0.001533138\n",
       "8681585128, 0.0015309591775458267, 0.001528971504045537, 0.0015271382880837822\n",
       ", 0.0015254293940398244, 0.0015238206426094324, 0.001522292633315865, 0.001520\n",
       "8297998059123, 0.001519419651909207, 0.0015180521675376296, 0.0015167193048015\n",
       "953, 0.0015154146105765267, 0.001514132906451322, 0.0015128700367609007, 0.001\n",
       "511622666428853, 0.0015103881187731464, 0.001509164245374279, 0.00150794932166\n",
       "76094, 0.0015067419631742704, 0.00150554105829067, 0.0015043457143633166, 0.00\n",
       "1503155214422452, 0.0015019689824675735, 0.0015007865556143222, 0.001499607561\n",
       "7463229, 0.0014984317015838526, 0.0014972587342963, 0.0014960884659578203, 0.0\n",
       "01494920740284331, 0.0014937554312008541, 0.0014925924368774481, 0.00149143167\n",
       "49434797, 0.0014902730786473764, 0.0014891165937749693, 0.0014879621761765135,\n",
       " 0.0014868097897822092, 0.001485659405009483, 0.0014845109974848793, 0.0014833\n",
       "64547018211, 0.0014822200367792162, 0.0014810774526367164, 0.00147993678262813\n",
       "9, 0.0014787980165337456, 0.0014776611455347658, 0.001476526161938967, 0.00147\n",
       "5393058960344, 0.0014742618305420927, 0.0014731324712146513, 0.001472004975981\n",
       "514, 0.0014708793402275712, 0.0014697555596455195, 0.0014686336301766665, 0.00\n",
       "14675135479634135, 0.0014663953093110094, 0.001465278910656883, 0.001464164348\n",
       "545924, 0.0014630516196105822, 0.0014619407205549437, 0.001460831648141824, 0.\n",
       "0014597243991824357, 0.001458618970528034, 0.0014575153590632216, 0.0014564135\n",
       "617004773, 0.001455313575375857, 0.00145421539704541, 0.0014531190236823535, 0\n",
       ".001452024452274764, 0.00145093167982373, 0.001449840703341808, 0.001448751519\n",
       "8518411, 0.0014476641263859423, 0.001446578519984668, 0.001445494697696415, 0.\n",
       "0014444126565768326, 0.0014433323936884237, 0.0014422539061001338, 0.001441177\n",
       "1908871285, 0.001440102245130493, 0.001439029065917058, 0.0014379576503392327,\n",
       " 0.0014368879954948393, 0.001435820098487066, 0.0014347539564242755, 0.0014336\n",
       "895664200178, 0.001432626925592889, 0.0014315660310665218, 0.00143050687996949\n",
       "85, 0.001429449469435343, 0.0014283937966024444, 0.0014273398586140754, 0.0014\n",
       "262876526182904, 0.001425237175767983, 0.001424188425220808, 0.001423141398139\n",
       "1678, 0.0014220960916902004, 0.0014210525030457785, 0.001420010629382459, 0.00\n",
       "14189704678815097, 0.0014179320157288503, 0.001416895270115071, 0.001415860228\n",
       "23542, 0.0014148268872897843, 0.0014137952444826496, 0.0014127652970231626, 0.\n",
       "0014117370421250378, 0.0014107104770066062, 0.0014096855988907843, 0.001408662\n",
       "4050050633, 0.0014076408925814993, 0.0014066210588567197, 0.001405602901071881\n",
       "1, 0.0014045864164727077, 0.0014035716023094542, 0.0014025584558368769, 0.0014\n",
       "015469743142745, 0.0014005371550054472, 0.0013995289951786944, 0.0013985224921\n",
       "068135, 0.0013975176430670914, 0.0013965144453412736, 0.001395512896215591, 0.\n",
       "0013945129929807498, 0.0013935147329318782, 0.0013925181133685835, 0.001391523\n",
       "131594892, 0.0013905297849192712, 0.0013895380706546176, 0.0013885479861182196\n",
       ", 0.0013875595286318224, 0.0013865726955215288, 0.0013855874841178578, 0.00138\n",
       "46038917557184, 0.001383621915774396, 0.0013826415535175304, 0.001381662802333\n",
       "168, 0.0013806856595736793, 0.001379710122595807, 0.0013787361887606184, 0.001\n",
       "3777638554335321, 0.001376793119984305, 0.001375823979786987, 0.00137485643221\n",
       "99803, 0.0013738904746659637, 0.001372926104511952, 0.0013719633191492152, 0.0\n",
       "013710021159733363, 0.001370042492384167, 0.001369084445785845, 0.001368127973\n",
       "5867674, 0.0013671730731995818, 0.0013662197420412, 0.0013652679775327753, 0.0\n",
       "013643177770996967, 0.0013633691381715863, 0.0013624220581822946, 0.0013614765\n",
       "345698824, 0.001360532564776616, 0.0013595901462489792, 0.0013586492764376594,\n",
       " 0.0013577099527974912, 0.0013567721727875443, 0.0013558359338710336, 0.001354\n",
       "9012335153483, 0.0013539680691920398, 0.001353036438376819, 0.0013521063385495\n",
       "476, 0.0013511777671942145, 0.0013502507217989647, 0.0013493251998560557, 0.00\n",
       "13484011988618635, 0.0013474787163168835, 0.001346557749725734, 0.001345638296\n",
       "5971057, 0.0013447203544438107, 0.001343803920782732, 0.0013428889931348385, 0\n",
       ".0013419755690251669, 0.0013410636459828338, 0.0013401532215410092, 0.00133924\n",
       "42932369136, 0.0013383368586118228, 0.001337430915211041, 0.001336526460583925\n",
       "6, 0.0013356234922838456, 0.001334722007868187, 0.0013338220048983783, 0.00133\n",
       "29234809398185, 0.0013320264335619275, 0.0013311308603381226, 0.00133023675884\n",
       "58061, 0.0013293441266663533, 0.0013284529613851184, 0.0013275632605914264, 0.\n",
       "0013266750218785654, 0.001325788242843778, 0.001324902921088248, 0.00132401905\n",
       "42171174, 0.0013231366398394435, 0.001322255675568224, 0.0013213761590203856, \n",
       "0.0013204980878167585, 0.0013196214595820926, 0.001318746271945035, 0.00131787\n",
       "25225381329, 0.001317000208997822, 0.0013161293289644224, 0.001315259880082148\n",
       ", 0.0013143918599990458, 0.0013135252663670678, 0.0013126600968420038, 0.00131\n",
       "17963490835054, 0.0013109340207550601, 0.0013100731095240063, 0.00130921361306\n",
       "15159, 0.0013083555290425747, 0.0013074988551459968, 0.0013066435890544158, 0.\n",
       "0013057897284542764, 0.0013049372710357973, 0.0013040862144930384, 0.001303236\n",
       "5565238194, 0.0013023882948297364, 0.001301541427116195, 0.0013006959510923387\n",
       ", 0.001299851864471091, 0.0012990091649691317, 0.0012981678503069098, 0.001297\n",
       "3279182085897, 0.0012964893664020886, 0.0012956521926190659, 0.001294816394594\n",
       "9041, 0.001293981970068702, 0.0012931489167832782, 0.0012923172324851634, 0.00\n",
       "12914869149245817, 0.0012906579618554776, 0.001289830371035461, 0.001289004140\n",
       "2258288, 0.0012881792671915707, 0.0012873557497013635, 0.0012865335855274974, \n",
       "0.0012857127724459746, 0.0012848933082364425, 0.0012840751906821797, 0.0012832\n",
       "58417570109, 0.0012824429866908167, 0.001281628895838501, 0.001280816142810976\n",
       ", 0.0012800047254096934, 0.0012791946414397054, 0.0012783858887096802, 0.00127\n",
       "75784650318847, 0.0012767723682221707, 0.001275967596100002, 0.001275164146488\n",
       "399, 0.0012743620172139813, 0.0012735612061069259, 0.0012727617110009888, 0.00\n",
       "12719635297334761, 0.0012711666601452419, 0.0012703711000807175, 0.00126957684\n",
       "73878369, 0.001268783899918105, 0.0012679922555265356, 0.0012672019120716803, \n",
       "0.0012664128674155985, 0.0012656251194238714, 0.001264838665965594, 0.00126405\n",
       "35049133433, 0.0012632696341432156, 0.0012624870515347759, 0.00126170575497109\n",
       "16, 0.0012609257423387044, 0.00126014701152762, 0.0012593695604313246, 0.00125\n",
       "85933869467584, 0.0012578184889743109, 0.001257044864417841, 0.001256272511184\n",
       "6462, 0.001255501427185449, 0.0012547316103344193, 0.0012539630585491509, 0.00\n",
       "1253195769750663, 0.0012524297418633845, 0.0012516649728151607, 0.001250901460\n",
       "5372425, 0.0012501392029642807, 0.0012493781980343075, 0.0012486184436887686, \n",
       "0.0012478599378724718, 0.0012471026785336146, 0.0012463466636237557, 0.0012455\n",
       "9189109782, 0.0012448383589141214, 0.001244086065034287, 0.001243335007423318,\n",
       " 0.0012425851840495586, 0.001241836592884692, 0.0012410892319037225, 0.0012403\n",
       "430990849931, 0.0012395981924101752, 0.0012388545098642486, 0.0012381120494354\n",
       "937, 0.001237370809115521, 0.0012366307868992212, 0.0012358919807847946, 0.001\n",
       "2351543887737268, 0.0012344180088707719, 0.0012336828390839937, 0.001232948877\n",
       "4247017, 0.0012322161219074884, 0.0012314845705502034, 0.001230754221373961, 0\n",
       ".0012300250724031193, 0.0012292971216652877, 0.0012285703671913119, 0.00122784\n",
       "48070152854, 0.00122712043917451, 0.0012263972617095444, 0.001225675272664142,\n",
       " 0.0012249544700852822, 0.0012242348520231474, 0.00122351641653113, 0.00122279\n",
       "91616658293, 0.0012220830854870015, 0.0012213681860576502, 0.00122065446144392\n",
       "4, 0.0012199419097151422, 0.0012192305289438232, 0.0012185203172056403, 0.0012\n",
       "178112725794294, 0.001217103393147176, 0.0012163966769940398, 0.00121569112220\n",
       "8308, 0.0012149867268814072, 0.001214283489107923, 0.0012135814069855434, 0.00\n",
       "12128804786151047, 0.001212180702100563, 0.0012114820755489673, 0.001210784597\n",
       "0705093, 0.001210088264778468, 0.0012093930767892189, 0.0012086990312222467, 0\n",
       ".0012080061262001167, 0.0012073143598484895, 0.0012066237302960957, 0.00120593\n",
       "4235674743, 0.0012052458741193149, 0.0012045586437677575, 0.001203872542761073\n",
       "9, 0.0012031875692433231, 0.0012025037213616164, 0.0012018209972661116, 0.0012\n",
       "011393951100004]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-86125cd57f0fdb89",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "You can evaluate the learning process by monitoring the loss as training progress. In the following graph, we visualize the loss as a function of the iterations. This is possible since we are saving the loss value at every iteration in the `J_history` array. This visualization might help you find problems with your code. Notice that since the network converges quickly, we are using logarithmic scale for the number of iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a565f1f721f6377f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0YAAAH0CAYAAAAZuT1PAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABEyUlEQVR4nO3deXhU5d3G8fs32RMgYUf2XWSRLWyKu7VaFdxREVFB3JCqtda2trW+1VZttUVxQRQQF3CrolKtdQVUJCC7ILssskNCAtmf948ZNMQAATI5M3O+n+vKlZmzPOeeSRhze848Y845AQAAAICfBbwOAAAAAABeoxgBAAAA8D2KEQAAAADfoxgBAAAA8D2KEQAAAADfoxgBAAAA8D2KEQAgLCxovJntNLOvqvnY/zGzodV5zNBx/2Jm28xsUwXrTjKzZdWdqVyG35nZOC8zAECkMj7HCACqj5mtkTTcOfc/r7OEm5mdJOllScc65/LCeJx7JbV1zl0VrmNUMkdzScsktXDObanE9msUxt8FMztV0gvOuabhGB8AYg1njAAA4dJC0ppwlqII01zS9sqUoqMVOhvHf8MBoArxogoAEcDMkszsn2a2MfT1TzNLCq2rZ2bvmNkuM9thZtP3/VFsZr8xsw1mttvMlpnZGQcY/1wz+9rMcsxsXegsy751yWb2gpltDx1jtpk1PMA4d5vZytDxlpjZhQfYbpikcZL6mVmumf3ZzK4xsxnltnNm1jZ0e4KZjTGzd0PjzzKzNmW27WRmH4Seg82hy8LOlvQ7SYNCx5kf2vYTMxseuh0ws3vMbK2ZbTGz580sPbSuZSjDUDP7LnQZ3O8P8nNKD+2/NTTePaHxz5T0gaTGoRwTKtj3VDNbH7o9ScEi9XZo+7tCy/ua2eehn8P80Fmffft/Ymb3m9lMSXsktTaza83sm9DztcrMbghtmybpP2Xy5JpZYzO718xeKDPmADNbHDreJ2Z2XJl1a8zsTjNbYGbZZjbFzJJD6w74OwkA0YoXMQCIDL+X1FdSN0ldJfWWdE9o3a8krZdUX1JDBYuAM7NjJY2U1Ms5V1PSzyWtOcD4eZKulpQh6VxJN5nZBaF1QyWlS2omqa6kGyXtPcA4KyWdFNr+z5JeMLNjym/knHs2NM4Xzrkazrk/HeLx73N5aNzaklZIul+SzKympP9Jek9SY0ltJX3onHtP0gOSpoSO07WCMa8JfZ0mqbWkGpIeL7dNf0nHSjpD0h/LFoRyHlPwsbeWdIqCz+m1ocvhzpG0MZTjmoM9SOfcEEnfSTo/tP1DZtZE0ruS/iKpjqQ7Jb1uZvXL7DpE0ghJNSWtlbRF0nmSakm6VtKjZtYjdJaubJ4azrmNZTOYWXsFL3W8TcHfrWkKFrXEMptdJulsSa0kHa/g8ygd4HfyYI8ZACIdxQgAIsNgSfc557Y457YqWA6GhNYVSTpGwfeuFDnnprvgG0RLJCVJ6mhmCc65Nc65lRUN7pz7xDm30DlX6pxboOAfxKeUGb+ugu/TKXHOzXHO5RxgnFedcxtD40yRtFzBEldV/u2c+8o5VyzpRQWLohT843+Tc+4fzrl859xu59ysSo45WNIjzrlVzrlcSb+VdLmZxZfZ5s/Oub3OufmS5itYTvdjZnEKFrffho6/RtI/9OPP6WhdJWmac25a6Pn9QFKWpF+U2WaCc26xc6449LvwrnNupQv6VNJ/FSyulTFI0rvOuQ+cc0WS/i4pRdIJZbYZHfp575D0tn78eRzodxIAohbFCAAiQ2MFzwDssza0TJIeVvDsyX9Dl0vdLUnOuRUK/t/+eyVtMbPJZtZYFTCzPmb2cegSsGwFz+bUC62eJOl9SZMteBnfQ2aWcIBxrjazeaFLqHZJ6lxmnKpQdja3PQqe3ZGCZ7MqLH2VUNFzG6/gmY5DHbesepISKhiryRHmKq+FpEv3Pbeh57e/ggVkn3VldzCzc8zsy9DlbLsULFGV/Xns97w450pD45d9PAd6Xir8nQSAaEYxAoDIsFHBP4z3aR5aptDZiV8551pLGiDpDgu9l8g595Jzrn9oXyfpwQOM/5KkqZKaOefSJT0lyUJjFDnn/uyc66jg2YLzFLxEbD9m1kLSMwpevlfXOZchadG+cSohT1JqmfEaVXI/KfgHe+sDrDvUmYqKnttiSZsP4/iStE3BMyXlx9pwmOPsUz73OkmTnHMZZb7SnHN/q2gfC74H7XUFz/Q0DP08punHn8dhPS9mZgoW0EM+noP9TgJAtKIYAUD1S7DghAf7vuIVvLTtHjOrb2b1JP1R0guSZGbnmVnb0B+u2QpeQldqZsea2emhP5DzFXxfUOkBjllT0g7nXL6Z9ZZ05b4VZnaamXUJXSqWo+Af/xWNk6bgH9tbQ/tdq+AZo8qaL6mTmXULvYn/3sPY9x1Jx5jZbRacqKKmmfUJrdssqeVB3vz/sqTbzayVmdXQj+9JKj6M48s5VyLpFUn3h47fQtIdCv2cjsBm7V/2XpB0vpn93MziQr8bp5rZgabbTlTwUsqtkorN7BxJZ5Ubv66FJpqowCuSzjWzM0JnCH8lqUDS54cKfqDfyUPtBwCRjGIEANVvmoIlZt/XvQq+4T5L0gJJCyXNDS2TpHYKTjyQK+kLSU845z5W8I/ivyl4JmOTpAYKvn+mIjdLus/MditYul4ps66RpNcULEXfSPpUwcvr9uOcW6Lge2q+UPCP7i6SZlb2QTvnvpV0X+ixLJc04+B77Lfvbkk/k3S+go91uYKTKUjSq6Hv281sbgW7P6fg4/lM0moFS+StlT12ObcqeOZrlYL5XwqNfyT+qmAZ3mVmdzrn1kkaqOBEBlsVPIP0ax3gv9Wh52SUgj/LnQqW3all1i9VsBSuCh2jcbn9lyn4vqbHFPwdOl/BySAKK5H9QL+TABC1+IBXAAAAAL7HGSMAAAAAvkcxAgAAAOB7FCMAAAAAvkcxAgAAAOB7FCMAAAAAvhfvdYCqUq9ePdeyZUuvYwAAAACIYHPmzNnmnKtffnnMFKOWLVsqKyvL6xgAAAAAIpiZra1oOZfSAQAAAPA9ihEAAAAA36MYAQAAAPA9ihEAAAAA36MYAQAAAPA9ihEAAAAA36MYAQAAAPA9ihEAAAAA36MYAQAAAPA9ihEAAAAA36MYAQAAAPA9ihEAAAAA36MYAQAAAPC9sBYjMzvbzJaZ2Qozu7uC9Seb2VwzKzazS8qtG2pmy0NfQ8OZEwAAAIC/ha0YmVmcpDGSzpHUUdIVZtax3GbfSbpG0kvl9q0j6U+S+kjqLelPZlY7XFkBAAAA+Fs4zxj1lrTCObfKOVcoabKkgWU3cM6tcc4tkFRabt+fS/rAObfDObdT0geSzg5jVgAAAAA+Fs5i1ETSujL314eWVdm+ZjbCzLLMLGvr1q1HHBQAAACAv0X15AvOubHOuUznXGb9+vW9jgMAAAAgSoWzGG2Q1KzM/aahZeHeFwAAAAAOSziL0WxJ7cyslZklSrpc0tRK7vu+pLPMrHZo0oWzQssAAAAAoMqFrRg554oljVSw0Hwj6RXn3GIzu8/MBkiSmfUys/WSLpX0tJktDu27Q9L/KViuZku6L7QMAAAAAKqcOee8zlAlMjMzXVZWltcxAAAAAEQwM5vjnMssvzyqJ18AAAAAgKpAMQIAAADgexQjAAAAAL5HMQIAAADgexQjAAAAAL5HMQIAAADgexQjAAAAAL5HMQIAAADgexQjAAAAAL5HMQIAAADgexQjAAAAAL5HMQIAAADgexQjAAAAAL5HMQIAAADgexQjAAAAAL5HMQIAAADgexQjAAAAAL5HMQIAAADgexQjAAAAAL5HMQIAAADgexQjAAAAAL5HMQIAAADgexQjAAAAAL5HMQIAAADgexQjAAAAAL5HMQIAAADgexQjAAAAAL5HMQIAAADgexQjAAAAAL5HMQIAAADgexQjAAAAAL5HMQIAAADgexQjAAAAAL5HMQIAAADgexQjAAAAAL5HMQIAAADgexQjAAAAAL5HMQIAAADgexQjAAAAAL5HMQIAAADgexQjAAAAAL5HMQIAAADgexQjAAAAAL5HMQIAAADgexQjAAAAAL5HMQIAAADgexQjAAAAAL5HMQIAAADgexQjAAAAAL5HMQIAAADgexQjAAAAAL5HMQIAAADgexQjAAAAAL5HMQIAAADgexQjAAAAAL5HMQIAAADgexQjAAAAAL5HMQIAAADgexQjAAAAAL5HMQIAAADgexQjAAAAAL5HMQIAAADgexQjAAAAAL5HMQIAAADgexQjAAAAAL5HMQIAAADgexQjAAAAAL5HMQIAAADgexQjAAAAAL5HMQIAAADgexQjAAAAAL4X1mJkZmeb2TIzW2Fmd1ewPsnMpoTWzzKzlqHlCWY20cwWmtk3ZvbbcOYEAAAA4G/x4RrYzOIkjZH0M0nrJc02s6nOuSVlNhsmaadzrq2ZXS7pQUmDJF0qKck518XMUiUtMbOXnXNrDnbMklJX+XyH9WgkO8wd7HB3AAAAAOCZsBUjSb0lrXDOrZIkM5ssaaCkssVooKR7Q7dfk/S4BRuFk5RmZvGSUiQVSso52MEWbshWm99Nq9IHcDTO6dxIf7v4eKWnJHgdBQAAAMAhhLMYNZG0rsz99ZL6HGgb51yxmWVLqqtgSRoo6XtJqZJud87tONjBGtZK1h0/a1+pYK7yJ5aC2+vwdti1p0gvfLlWizZO1xNX9lSXpumHd0AAAAAA1Sqcxeho9JZUIqmxpNqSppvZ//adfdrHzEZIGiFJzZs316gz2lV70AM5v2tjjXxpri5+8nP94fyOuqpPcy6vAwAAACJUOCdf2CCpWZn7TUPLKtwmdNlcuqTtkq6U9J5zrsg5t0XSTEmZ5Q/gnBvrnMt0zmXWr18/DA/hyPVsUVvvjjpJJ7Stqz+8uUi/nDxPuQXFXscCAAAAUIFwFqPZktqZWSszS5R0uaSp5baZKmlo6PYlkj5yzjlJ30k6XZLMLE1SX0lLw5g1LOqkJeq5ob1051nt9c6CjRrw+Awt27Tb61gAAAAAyglbMXLOFUsaKel9Sd9IesU5t9jM7jOzAaHNnpVU18xWSLpD0r4pvcdIqmFmixUsWOOdcwvClTWcAgHTyNPb6YXhfZSzt1gDx8zQ63PWex0LAAAAQBnmDncmggiVmZnpsrKyvI5xUFty8nXry19r1uodGpTZTH8e2EnJCXFexwIAAAB8w8zmOOd+8jadsH7AK/bXoFayXhzeR7ec1kZTstbpgjEztXpbntexAAAAAN+jGFWz+LiAfv3zDhp/bS9tysnX+Y/N0LsLvvc6FgAAAOBrFCOPnHZsA7076iS1a1hDt7w0V/dOXazC4lKvYwEAAAC+RDHyUJOMFE0Z0U/XndhKEz5fo0uf/kLrd+7xOhYAAADgOxQjjyXGB/TH8zvqycE9tGpLrs4dPUMfLd3sdSwAAADAVyhGEeKcLsfo7Vv7q0lGiq6bkKUH31uq4hIurQMAAACqA8UogrSsl6Y3bj5BV/Ruric/Wakrx83S5px8r2MBAAAAMY9iFGGSE+L014u66NFBXbVwfbbOHT1dM1ds8zoWAAAAENMoRhHqwu5NNXXkicpITdRVz87S6A+Xq7Q0Nj6MFwAAAIg0FKMI1q5hTb11y4ka2LWxHvngWw0d/5W25xZ4HQsAAACIORSjCJeWFK9HB3XTAxd20azVO3Tu6Bmas3aH17EAAACAmEIxigJmpiv7NNcbN52gxPiABj39pZ75bJWc49I6AAAAoCpQjKJI5ybpemdUf51xXAPdP+0bjZg0R9l7i7yOBQAAAEQ9ilGUqZWcoKeu6qk/nNdRHy/dovMem66F67O9jgUAAABENYpRFDIzDevfSlNu6KfiEqeLn/xck75cy6V1AAAAwBGiGEWxni1q691RJ6lfm7r6w5uL9MvJ85RbUOx1LAAAACDqUIyiXJ20RI2/ppfuPKu93lmwUQMen6Flm3Z7HQsAAACIKhSjGBAImEae3k4vDO+jnL3FGjhmhl6fs97rWAAAAEDUoBjFkBPa1NO0Uf3VtWmGfvXqfP3mtQXKLyrxOhYAAAAQ8ShGMaZBrWS9OLyPbjmtjaZkrdMFY2Zq9bY8r2MBAAAAEY1iFIPi4wL69c87aPw1vbQpJ1/nPzZD7y743utYAAAAQMSiGMWw0zo00LujTlLbBjV0y0tzde/UxSosLvU6FgAAABBxKEYxrklGil65oZ+uO7GVJny+Rpc+/YXW79zjdSwAAAAgolCMfCAxPqA/nt9RTw7uoVVbcnXu6Bn6aOlmr2MBAAAAEYNi5CPndDlGb9/aX00yUnTdhCw9+N5SFZdwaR0AAABAMfKZlvXS9MbNJ+iK3s315CcrdeW4Wdqck+91LAAAAMBTFCMfSk6I018v6qJHB3XVwvXZOnf0dM1csc3rWAAAAIBnKEY+dmH3pnpr5InKSE3UVc/O0ugPl6u01HkdCwAAAKh2FCOfa9+wpt665UQN7NpYj3zwrYaO/0rbcwu8jgUAAABUK4oRlJYUr0cHddMDF3bRrNU7dO7oGZqzdofXsQAAAIBqQzGCJMnMdGWf5nrjphOUGB/QoKe/1DOfrZJzXFoHAACA2Ecxwn46N0nXO6P664zjGuj+ad9oxKQ5yt5b5HUsAAAAIKwoRviJWskJeuqqnrrn3OP08dItOu+x6Vq4PtvrWAAAAEDYUIxQITPT8JNaa8oN/VRc4nTxk59r0pdrubQOAAAAMYlihIPq2aK23h11kvq1qas/vLlIv5w8T3kFxV7HAgAAAKoUxQiHVCctUeOv6aU7z2qvdxZs1IDHZ2jZpt1exwIAAACqDMUIlRIImEae3k4vDO+j7L3FGjhmhl6fs97rWAAAAECVoBjhsJzQpp6mjeqvrk0z9KtX5+s3ry1QflGJ17EAAACAo0IxwmFrUCtZLw7vo1tOa6MpWet0wZiZWr0tz+tYAAAAwBGjGOGIxMcF9Oufd9D4a3ppU06+zn9sht5d8L3XsQAAAIAjQjHCUTmtQwO9O+oktW1QQ7e8NFf3Tl2swuJSr2MBAAAAh4VihKPWJCNFr9zQT9ed2EoTPl+jS5/+Qut37vE6FgAAAFBpFCNUicT4gP54fkc9ObiHVm3J1bmjZ+ijpZu9jgUAAABUCsUIVeqcLsfo7Vv7q3FGiq6bkKUH31uq4hIurQMAAEBkoxihyrWsl6Z/33yCrujdTE9+slJXjpulzTn5XscCAAAADohihLBITojTXy86Xo9c1lUL12fr3NHTNXPFNq9jAQAAABWiGCGsLurRVG+NPFEZqYm66tlZGv3hcpWWOq9jAQAAAPuhGCHs2jesqbduOVEDuzbWIx98q6Hjv9L23AKvYwEAAAA/oBihWqQlxevRQd30wIVdNGv1Dp07eobmrN3hdSwAAABAEsUI1cjMdGWf5nrjphOUGB/QoKe/1DOfrZJzXFoHAAAAb1GMUO06N0nX27f21xnHNdD9077RiElzlL23yOtYAAAA8DGKETyRnpKgp67qqXvOPU4fL92i8x6broXrs72OBQAAAJ+iGMEzZqbhJ7XWlBv6qbjE6eInP9ekL9dyaR0AAACqHcUInuvZorbeHXWS+rWpqz+8uUi/nDxPeQXFXscCAACAj1CMEBHqpCVq/DW9dOdZ7fXOgo0a8PgMLdu02+tYAAAA8AmKESJGIGAaeXo7vTC8j7L3FmvgmBl6a94Gr2MBAADAByhGiDgntKmnaaP66/gmGbptyjzKEQAAAMKOYoSI1KBWsiZe11u9W9bRHa/M13uLNnkdCQAAADGMYoSIlZIYp2ev6aXjm6br1pfn6uNlW7yOBAAAgBhFMUJEq5EUrwnX9lb7hjV146Q5+nzlNq8jAQAAIAZRjBDx0lMSNGlYHzWvk6rhE7M0Z+0OryMBAAAgxlCMEBXqpCXqxeF91KBmkq55brYWrs/2OhIAAABiCMUIUaNBrWS9eH1f1UpJ0JDnZvE5RwAAAKgyFCNElSYZKXrp+j5Kig9o8LhZWrU11+tIAAAAiAEUI0SdFnXT9OLwPnLOafC4WVq3Y4/XkQAAABDlKEaISm0b1NSkYX20p7BEV477Ut9n7/U6EgAAAKIYxQhRq2PjWnr+ut7amVekweNmaevuAq8jAQAAIEpRjBDVujbL0Phre+n7Xfka8uws7cwr9DoSAAAAohDFCFGvV8s6eubqTK3alqeh479STn6R15EAAAAQZShGiAn929XTk4N7aMnGHF07frbyCoq9jgQAAIAoQjFCzDjjuIYafUV3ff3dTl3/fJbyi0q8jgQAAIAoEdZiZGZnm9kyM1thZndXsD7JzKaE1s8ys5Zl1h1vZl+Y2WIzW2hmyeHMitjwiy7H6O+XdtUXq7brphfmqLC41OtIAAAAiAJhK0ZmFidpjKRzJHWUdIWZdSy32TBJO51zbSU9KunB0L7xkl6QdKNzrpOkUyXxxhFUykU9mur+C7ro42Vb9cvJX6u4hHIEAACAgwvnGaPeklY451Y55wolTZY0sNw2AyVNDN1+TdIZZmaSzpK0wDk3X5Kcc9udc1wXhUq7sk9z/eG8jvrPok2689X5Kil1XkcCAABABIsP49hNJK0rc3+9pD4H2sY5V2xm2ZLqSmovyZnZ+5LqS5rsnHsojFkRg4b1b6X8ohI9/P4yJSfE6a8XdVGwdwMAAAD7C2cxOhrxkvpL6iVpj6QPzWyOc+7DshuZ2QhJIySpefPm1R4Ske+W09pqT2Gxxny8UskJcfrT+R0pRwAAAPiJcF5Kt0FSszL3m4aWVbhN6H1F6ZK2K3h26TPn3Dbn3B5J0yT1KH8A59xY51ymcy6zfv36YXgIiAV3nnWsrjuxlSZ8vkYPvb9MznFZHQAAAPYXzmI0W1I7M2tlZomSLpc0tdw2UyUNDd2+RNJHLvhX6/uSuphZaqgwnSJpSRizIoaZmf5w3nG6sk9zPfnJSj3+0QqvIwEAACDChO1SutB7hkYqWHLiJD3nnFtsZvdJynLOTZX0rKRJZrZC0g4Fy5OcczvN7BEFy5WTNM059264siL2mZn+MrCz8gtL9I8PvlVKYpyGn9Ta61gAAACIEBYrlxVlZma6rKwsr2MgwhWXlGrU5K81beEm/eWCzrqqbwuvIwEAAKAaheYuyCy/PFInXwDCIj4uoH8O6q78ojm6581FSk6I0yU9m3odCwAAAB4L53uMgIiUGB/QE4N7qH/berrrtfl6Z8FGryMBAADAYxQj+FJyQpzGXt1TPVvU1m2T5+l/SzZ7HQkAAAAeohjBt1IT4/XcNb3UqXEt3fziXE1fvtXrSAAAAPAIxQi+VjM5QROv663W9dN0/fNZmrVqu9eRAAAA4AGKEXwvIzVRLwzvoyYZKbpuwmx9/d1OryMBAACgmlGMAEn1aiTpxeF9VbdGkoY+95UWb8z2OhIAAACqEcUICGmUnqwXh/dRjaR4DXn2Ky3fvNvrSAAAAKgmFCOgjGZ1UvXi9X0VFzANHjdLa7bleR0JAAAA1YBiBJTTql6aXhzeR0UlpRo8bpbW79zjdSQAAACEGcUIqED7hjU1aVgf5eQXafC4Wdqck+91JAAAAIQRxQg4gM5N0jXxut7atrtAg8fN0vbcAq8jAQAAIEwoRsBB9GheW89e00vrduzRVc9+pew9RV5HAgAAQBhQjIBD6Nu6rsZenamVW3J19fivtDufcgQAABBrKEZAJZzSvr4ev7K7Fm3I1rAJWdpbWOJ1JAAAAFQhihFQSWd1aqRHB3XT7LU7NGJSlvKLKEcAAACxgmIEHIYBXRvrwYuP1/Tl2zTypbkqKin1OhIAAACqAMUIOEyXZTbT/w3spP99s0W3TZmnklLndSQAAAAcpXivAwDRaEi/ltpbVKIHpi1VcnycHr7keAUC5nUsAAAAHCGKEXCERpzcRnsLS/Xo/75VckJAf7mgs8woRwAAANGIYgQchVFntNWeomI9/ekqpSTE6ffnHkc5AgAAiEIUI+AomJnuPruD8gtLNG7GaqUmxumOs471OhYAAAAOE8UIOEpmpj+d30l7i0o0+qMVSk6M082ntvU6FgAAAA4DxQioAoGA6a8XHa/8olI99N4ypSTE6doTW3kdCwAAAJVEMQKqSFzA9I/LuqqguER/fnuJUhLidHnv5l7HAgAAQCVU6nOMzCzNzAKh2+3NbICZJYQ3GhB9EuICGn1Fd53Svr5++++FevPrDV5HAgAAQCVU9gNeP5OUbGZNJP1X0hBJE8IVCohmSfFxenpIT/VpVUd3vjpfn6/c5nUkAAAAHEJli5E55/ZIukjSE865SyV1Cl8sILolJ8Tp6SGZalkvTTe9MFertuZ6HQkAAAAHUeliZGb9JA2W9G5oWVx4IgGxIT0lQeOv6aX4gOm6CbO1M6/Q60gAAAA4gMoWo9sk/VbSv51zi82staSPw5YKiBHN6qRq7NU9tTE7Xze8MEcFxSVeRwIAAEAFKlWMnHOfOucGOOceDE3CsM05NyrM2YCY0LNFHT18yfH6avUO/e6NRXLOeR0JAAAA5VR2VrqXzKyWmaVJWiRpiZn9OrzRgNgxsFsT/fKMdnp97no98clKr+MAAACgnMpeStfROZcj6QJJ/5HUSsGZ6QBU0m1nttOAro318PvLNG3h917HAQAAQBmVLUYJoc8tukDSVOdckSSuBwIOg5npoUuOV4/mGbp9yjzNW7fL60gAAAAIqWwxelrSGklpkj4zsxaScsIVCohVyQlxGnt1purXTNLwiVnasGuv15EAAACgyk++MNo518Q59wsXtFbSaWHOBsSkejWSNP6aXiooKtGwCbOVW1DsdSQAAADfq+zkC+lm9oiZZYW+/qHg2SMAR6Bdw5oaM7iHlm/J1aiXv1ZJKVemAgAAeKmyl9I9J2m3pMtCXzmSxocrFOAHJ7evr3sHdNJHS7foL+8u8ToOAACAr8VXcrs2zrmLy9z/s5nNC0MewFeG9G2h1Vvz9NzM1Wpdv4aG9G3hdSQAAABfquwZo71m1n/fHTM7URLvGgeqwO/PPU5ndGige6cu1qffbvU6DgAAgC9VthjdKGmMma0xszWSHpd0Q9hSAT4SFzD964ruateghka+OFffbt7tdSQAAADfqeysdPOdc10lHS/peOdcd0mnhzUZ4CM1kuL13DW9lJwYp+smzNa23AKvIwEAAPhKZc8YSZKccznOuX2fX3RHGPIAvtU4I0Xjrs7UttwCjXg+S/lFJV5HAgAA8I3DKkblWJWlACBJ6tosQ49c1k1zv9ulu15bIOeYxhsAAKA6HE0x4i82IAx+0eUY3XX2sZo6f6P++b/lXscBAADwhYNO121mu1VxATJJKWFJBEA3ndJGq7fm6V8fLleremm6oHsTryMBAADEtIMWI+dczeoKAuBHZqb7L+yi73bs0V2vLVDT2inKbFnH61gAAAAx62gupQMQRonxAT11VU81qZ2iEZPm6Lvte7yOBAAAELMoRkAEq52WqGeHZqqk1Om6ibOVvbfI60gAAAAxiWIERLjW9Wvoqat6as22PI18aa6KSkq9jgQAABBzKEZAFOjXpq4euKiLpi/fpj9NXcw03gAAAFXsoJMvAIgcl2U206qteXrq05VqXS9Nw09q7XUkAACAmEExAqLIXT8/Vmu25en+ad+oZd00ndmxodeRAAAAYgKX0gFRJBAwPTqomzo3TteoyV9r8cZsryMBAADEBIoREGVSEuM0bmim0lMSNHxilrbk5HsdCQAAIOpRjIAo1LBWssYNzVT23iINfz5LewtLvI4EAAAQ1ShGQJTq1Dhdoy/vroUbsnX7lHkqLWWmOgAAgCNFMQKi2JkdG+r3vzhO7y3epIfeX+Z1HAAAgKjFrHRAlBvWv5VWbQtN410/TZdlNvM6EgAAQNShGAFRzsz05wGdtG7HHv3ujYVqVjtV/drU9ToWAABAVOFSOiAGJMQF9PiVPdSyXppufGGOVm3N9ToSAABAVKEYATEiPSVBzw3tpbiAadjELO3MK/Q6EgAAQNSgGAExpHndVI0d0lMbdu7VjS/MUWFxqdeRAAAAogLFCIgxmS3r6KFLjtes1Tv0u38vlHNM4w0AAHAoTL4AxKALujfRqm15Gv3hcrWun6abT23rdSQAAICIRjECYtTtZ7bTmm15eui9ZWpVN03ndDnG60gAAAARi0vpgBhlZnrokuPVo3mGbn9lnhZtyPY6EgAAQMSiGAExLDkhTk8PyVSd1ERd/3yWtu4u8DoSAABARKIYATGufs0kjb06Uzv3FOrGF+aooLjE60gAAAARh2IE+EDnJun6x6XdNGftTt3z70XMVAcAAFAOxQjwiXOPP0ajTm+rV+es1/iZa7yOAwAAEFHCWozM7GwzW2ZmK8zs7grWJ5nZlND6WWbWstz65maWa2Z3hjMn4Be3ndleZ3VsqL+8u0TTl2/1Og4AAEDECFsxMrM4SWMknSOpo6QrzKxjuc2GSdrpnGsr6VFJD5Zb/4ik/4QrI+A3gYDp0UHd1L5hTY186Wut3pbndSQAAICIEM4zRr0lrXDOrXLOFUqaLGlguW0GSpoYuv2apDPMzCTJzC6QtFrS4jBmBHwnLSlez1ydqYBJ1z+fpZz8Iq8jAQAAeC6cxaiJpHVl7q8PLatwG+dcsaRsSXXNrIak30j6cxjzAb7VrE6qnhjcU2u25em2yfNUUspkDAAAwN8idfKFeyU96pzLPdhGZjbCzLLMLGvrVt4vARyOfm3q6t4BnfTR0i16+P1lXscBAADwVHwYx94gqVmZ+01DyyraZr2ZxUtKl7RdUh9Jl5jZQ5IyJJWaWb5z7vGyOzvnxkoaK0mZmZn8L2/gMF3Vt4WWbsrRU5+uVIdGNXVB9/IndQEAAPwhnMVotqR2ZtZKwQJ0uaQry20zVdJQSV9IukTSRy74ASsn7dvAzO6VlFu+FAGoGn86v5NWbMnVXa8vUKt6aeraLMPrSAAAANUubJfShd4zNFLS+5K+kfSKc26xmd1nZgNCmz2r4HuKVki6Q9JPpvQGEF4JcQE9MbinGtRM0ohJWdqck+91JAAAgGpnwRM00S8zM9NlZWV5HQOIWks35eiiJz5Xu4Y1NWVEXyUnxHkdCQAAoMqZ2RznXGb55ZE6+QKAatahUS09clk3zV+3S797Y6Fi5X+aAAAAVAbFCMAPzu7cSHf8rL3e+HqDnpm+yus4AAAA1YZiBGA/t57eVud2OUZ//c9Sfbx0i9dxAAAAqgXFCMB+zEwPX3q8jmtUS6Ne/lorthz048QAAABiAsUIwE+kJsbrmaGZSowP6Prns5S9p8jrSAAAAGFFMQJQoSYZKXpqSE+t37lHI1+eq+KSUq8jAQAAhA3FCMAB9WpZR3+5oLOmL9+mv/5nqddxAAAAwibe6wAAItugXs31zfe79eyM1Tq2UU1dltnM60gAAABVjjNGAA7pnnOPU/+29XTPvxdpztqdXscBAACochQjAIcUHxfQ41d21zEZybph0hx9n73X60gAAABVimIEoFIyUhM17upM5ReVaMTzc7S3sMTrSAAAAFWGYgSg0to1rKl/Xd5NizZm667XF8g553UkAACAKkExAnBYzjiuoe76eQe9PX+jnvhkpddxAAAAqgSz0gE4bDee0lpLN+Xo7/9dpvYNa+pnHRt6HQkAAOCocMYIwGEzMz148fHq0iRdt03+Wt9u3u11JAAAgKNCMQJwRJIT4jR2SKZSk+I1fGKWduYVeh0JAADgiFGMAByxRunJGjukpzbl5OvmF+eqqKTU60gAAABHhGIE4Kh0b15bf72wi75YtV1/eWeJ13EAAACOCJMvADhqF/dsqmWbd2vsZ6vUvlFNDe7TwutIAAAAh4UzRgCqxG/O7qBTj62vP721WJ+v3OZ1HAAAgMNCMQJQJeICptFXdFeremm66YW5Wr0tz+tIAAAAlUYxAlBlaiUn6NmhvRQwadjE2creW+R1JAAAgEqhGAGoUs3rpuqpq3pq3Y49GvnSXBUzUx0AAIgCFCMAVa5P67q6/4Iumr58m/6PmeoAAEAUYFY6AGFxWa9mWr5lt56ZvlptG9TQkH4tvY4EAABwQJwxAhA2d59znE7v0ED3vr1EM5YzUx0AAIhcFCMAYRMXMP3r8m5qW7+Gbn5xjlZtzfU6EgAAQIUoRgDCqmZygsYNzVRCXEDDJmYpew8z1QEAgMhDMQIQds3qpOqpIT21Yede3fzSHBUxUx0AAIgwFCMA1aJXyzp64KIumrliu+6duljOOa8jAQAA/IBZ6QBUm0t6NtXyLbv19Ker1L5hTQ09oaXXkQAAACRxxghANbvr5x105nEN9ee3F+uzb7d6HQcAAEASxQhANYsLmP55eTe1b1hTt7w0Vyu2MFMdAADwHsUIQLWrkRSvcUMzlRQf0LCJs7Uzr9DrSAAAwOcoRgA80bR2qp4ekqnvd+XrphfnqLCYmeoAAIB3KEYAPNOzRW397eIu+nLVDv1p6iJmqgMAAJ5hVjoAnrqoR1Ot2JKrJz5ZqXYNauq6/q28jgQAAHyIM0YAPHfnWcfqrI4N9Zd3l+jjZVu8jgMAAHyIYgTAc4GA6dFB3dShUS2NeulrLd+82+tIAADAZyhGACJC2r6Z6hLiNGxilnYwUx0AAKhGFCMAEaNxRorGXt1Tm3LydeMLzFQHAACqD8UIQETp0by2Hr7keH21eofueXMhM9UBAIBqwax0ACLOwG5NtGJLrh77aIXaN6yp4Se19joSAACIcRQjABHp9jPba8WWXN0/7Ru1rp+m0zs09DoSAACIYVxKByAiBQKmf1zWVR2PqaVbX/paizdmex0JAADEMIoRgIiVmhivZ4f2Uq2UBF03YbY27NrrdSQAABCjKEYAIlqj9GSNv7aX9hSU6NrxXyl7b5HXkQAAQAyiGAGIeB0a1dLTQ3pq9bY83TApSwXFJV5HAgAAMYZiBCAqnNC2nh6+pKu+XLVDd722QKWlTOMNAACqDrPSAYgaF3Rvog279urh95epcUaKfnN2B68jAQCAGEExAhBVbj61jTbs2qsnP1mpxhkpGtK3hdeRAABADKAYAYgqZqb7BnTS5ux8/emtRWpUK1k/68hnHAEAgKPDe4wARJ34uIAeu7K7ujRJ160vz9W8dbu8jgQAAKIcxQhAVEpNjNe4ob1Uv2aShk2YrbXb87yOBAAAohjFCEDUql8zSROv7a1S53TN+NnakVfodSQAABClKEYAolrr+jU0bmimNu7aq2ETZ2tvIZ9xBAAADh/FCEDU69mijv51eTfNW7dLv5z8tUr4jCMAAHCYKEYAYsLZnY/RH8/rqP8u2az/e2eJnKMcAQCAymO6bgAx49oTW2nDzr0aN2O1mmSk6PqTW3sdCQAARAmKEYCY8rtfHKfvc/J1/7Rv1Cg9Wed3bex1JAAAEAUoRgBiSiBg+selXbU1p0C/emW+GtRMUp/Wdb2OBQAAIhzvMQIQc5IT4jT26p5qVidF1z+fpRVbdnsdCQAARDiKEYCYlJGaqAnX9lZSQpyGPjdbW3LyvY4EAAAiGMUIQMxqVidV46/ppZ17CnXthNnKLSj2OhIAAIhQFCMAMa1zk3SNGdxDSzft1i0vzlVRSanXkQAAQASiGAGIeacd20APXNhZn367Vb9+db5K+QBYAABQDrPSAfCFQb2aa1tuoR5+f5lSEuP1wIWdZWZexwIAABGCYgTAN245ra32FBZrzMcrlZYYp9+fexzlCAAASKIYAfCZO886VnkFJRo3Y7XSkuJ1+8/aex0JAABEAIoRAF8xM/3xvI7KKyjWvz5crhpJ8br+5NZexwIAAB6jGAHwnUDA9LeLj9eeohLdP+0bpSTG6aq+LbyOBQAAPEQxAuBLcQHTo5d1U35hif7w1iKlJcXpwu5NvY4FAAA8Etbpus3sbDNbZmYrzOzuCtYnmdmU0PpZZtYytPxnZjbHzBaGvp8ezpwA/CkxPqAxg3uoX+u6uvPVBXpv0SavIwEAAI+ErRiZWZykMZLOkdRR0hVm1rHcZsMk7XTOtZX0qKQHQ8u3STrfOddF0lBJk8KVE4C/JSfE6ZmrM9W1abpufXmuPv12q9eRAACAB8J5xqi3pBXOuVXOuUJJkyUNLLfNQEkTQ7dfk3SGmZlz7mvn3MbQ8sWSUswsKYxZAfhYWlK8xl/bW+0a1NQNk7I0a9V2ryMBAIBqFs5i1ETSujL314eWVbiNc65YUrakuuW2uVjSXOdcQZhyAoDSUxI0aVhvNa2dqmETszR/3S6vIwEAgGoU1vcYHS0z66Tg5XU3HGD9CDPLMrOsrVu5/AXA0albI0kvDOuj2mkJuvq5r7R0U47XkQAAQDUJZzHaIKlZmftNQ8sq3MbM4iWlS9oeut9U0r8lXe2cW1nRAZxzY51zmc65zPr161dxfAB+1Cg9WS8N76uUhDhdNe4rrdqa63UkAABQDcJZjGZLamdmrcwsUdLlkqaW22aqgpMrSNIlkj5yzjkzy5D0rqS7nXMzw5gRAH6iWZ1UvTC8j5xzumrcLK3fucfrSAAAIMzCVoxC7xkaKel9Sd9IesU5t9jM7jOzAaHNnpVU18xWSLpD0r4pvUdKaivpj2Y2L/TVIFxZAaC8tg1q6PlhvZVbUKzB42ZpS06+15EAAEAYmXPO6wxVIjMz02VlZXkdA0CMmfvdTl01bpaa1k7R5BH9VCct0etIAADgKJjZHOdcZvnlET35AgB4rUfz2ho3NFNrt+/R0Oe+UvaeIq8jAQCAMKAYAcAhnNCmnp66qqeWbdqtK575Uttz+fQAAABiDcUIACrhtA4N9MzQTK3cmqtBY7/UZt5zBABATKEYAUAlndK+viZe11vf79qry57+gtnqAACIIRQjADgMfVvX1aThfbQzr1CXPfWFVm/L8zoSAACoAhQjADhMPZrX1ssj+iq/uFSXPf2Fvt282+tIAADgKFGMAOAIdGqcrikj+sokDXr6Cy3akO11JAAAcBQoRgBwhNo1rKlXbuin1MR4XfHMl5qzdqfXkQAAwBGiGAHAUWhZL02v3NhPddMSNeTZWfpi5XavIwEAgCNAMQKAo9QkI0Wv3NBPTTJSdM34r/TJsi1eRwIAAIeJYgQAVaBBrWRNuaGf2jaooeufz9J7izZ5HQkAABwGihEAVJE6aYl66fq+6tIkXbe8NFdvzdvgdSQAAFBJFCMAqELpKQmaNKyPereso9umzNPkr77zOhIAAKgEihEAVLG0pHiNv7aXTmlfX3e/sVDPzVjtdSQAAHAIFCMACIPkhDg9PaSnzu7USPe9s0R/+89SlZY6r2MBAIADoBgBQJgkxcfp8Su766q+zfXUpys18uW5yi8q8ToWAACoAMUIAMIoPi6g/xvYWfece5z+s2iTLh/7pbblFngdCwAAlEMxAoAwMzMNP6m1nhzcU0s35eiCMTO1fPNur2MBAIAyKEYAUE3O7txIU0b0U35RqS568nPNXLHN60gAACCEYgQA1ahrswy9ecsJOiY9WUOf+0qvzF7ndSQAACCKEQBUu6a1U/XaTSeoX5u6uuv1BXroPWasAwDAaxQjAPBAreQEPXdNL13Ru5me+GSlbp38NTPWAQDgoXivAwCAXyXEBfTAhV3Usm6a/vqfpfp+1149c3Wm6tZI8joaAAC+wxkjAPCQmemGU9roicE9tHhjji584nOt2JLrdSwAAHyHYgQAEeAXXY7R5BF9taewWBc9MZMZ6wAAqGYUIwCIEN2b19a/bz5RDWsla8izs/TYh8uZlAEAgGpCMQKACNKsTqrevOVEnd+1sf7xwbe6dsJs7cgr9DoWAAAxj2IEABEmLSle/xzUTX+5oLO+WLld542errnf7fQ6FgAAMY1iBAARyMx0Vd8Wev2mExQXZ7rsqS/03IzVco5L6wAACAeKEQBEsC5N0/XOyJN0WocGuu+dJbr5xbnKyS/yOhYAADGHYgQAES49NUFjh/TU737RQf9dslkDHpuhJRtzvI4FAEBMoRgBQBQwM404uY0mj+irvUUluvCJmZoy+zsurQMAoIpQjAAgivRqWUfvjjpJvVrW0W9eX6g7X12gvYUlXscCACDqUYwAIMrUq5Gkidf11m1nttMbX6/XgMdnaMH6XV7HAgAgqlGMACAKxQVMt53ZXpOu66Pd+cW68InP9ff3l6mgmLNHAAAcCYoRAESx/u3q6f3bT9aF3Zvo8Y9XaMBjM7VoQ7bXsQAAiDoUIwCIcukpCfr7pV313DWZ2rmnUAPHzNQjH3yrwuJSr6MBABA1KEYAECNO79BQH9x+igZ2a6zRHy7XwDEztXgjZ48AAKgMihEAxJD01AQ9clk3jbs6U9tyCzTw8Zn65/++VVEJZ48AADgYihEAxKAzOzbUB7efrPO7NtY//7dcAx+fyYfCAgBwEBQjAIhRGamJenRQN40d0lNbdhdowOMz9MC0b7Q7v8jraAAARByKEQDEuLM6NdIHt5+si3o00TPTV+m0v3+q1+asV2mp8zoaAAARg2IEAD5QOy1RD13SVW/efKKa1UnRna/O14VPfq5563Z5HQ0AgIhAMQIAH+naLEOv33iCHrmsqzbu2qsLxszUna/O15bd+V5HAwDAUxQjAPCZQMB0UY+m+vjOU3XjKW301rwNOv3vn2rsZyv57CMAgG9RjADAp2okxevuczrov7efoj6t6uiBaUt19j8/0/+WbJZzvP8IAOAvFCMA8LlW9dL07DW9NP7aXpKk4c9n6YIxM/Xxsi0UJACAb1CMAACSpNOObaD3bz9ZD118vLbnFera8bN10ZOf67Nvt1KQAAAxz2LlP3aZmZkuKyvL6xgAEBMKi0v12pz1evyj5dqYna+eLWrrjp+11wlt6srMvI4HAMARM7M5zrnMnyynGAEADqSguESvZK3XmI9WaFNOvnq3qqPbz2yvfm3qeh0NAIAjQjECAByx/KISTZm9TmM+XqEtuwvUo3mGruvfSmd3aqT4OK7KBgBED4oRAOCo5ReVaPJX3+m5mWv03Y49apyerKtPaKnLezVTRmqi1/EAADgkihEAoMqUlDp9tHSLnpuxWl+s2q7khIAu7tFU157YUm0b1PQ6HgAAB0QxAgCExTff52j8zNV6c95GFRaX6uT29XV13xY69dj6XGYHAIg4FCMAQFhtzy3QS7O+0/NfrtXW3QWqVyNJF3ZvrEt6NtOxjTiLBACIDBQjAEC1KCop1SfLtuq1Oev04TdbVFzq1KVJui7p2VQDujZW7TTeiwQA8A7FCABQ7bbnFmjq/I16bc56Ld6Yo4Q40xkdGuq8rsfo1GMbqEZSvNcRAQA+QzECAHjqm+9z9Pqc9Xpz3gZtyy1UYnxAJ7erp593aqQzj2vImSQAQLWgGAEAIkJJqVPWmh16f/Fmvb94kzbs2qu4gKlPqzo6u3Mjnd6hgZrWTvU6JgAgRlGMAAARxzmnRRty9N7i7/Xeok1auTVPktS6XppOaldP/dvVV9/WdVQzOcHjpACAWEExAgBEvBVbcvXZt1s1fflWfblqh/YWlSg+YOrRvLb6t6un3q3qqGvTDKUkxnkdFQAQpShGAICoUlBcorlrd2n68q2avnybFm3MlnNSfMDUqUm6MlvUVq+WtdWzRR3Vr5nkdVwAQJSgGAEAotrOvELN/W6nstbu1Jw1OzV//S4VFJdKkprWTlHnxunq3KSWOjVJV+fG6ZQlAECFDlSMmCcVABAVaqcl6ozjGuqM4xpKkgqLS7VoY/YPJWnxxhy9t3jTD9s3rJWk446ppfYNa6ptgxo/fGeKcABARfivAwAgKiXGB9SjeW31aF77h2W784u0ZGOOFm3M0aIN2Vq6abc+X7ldhaEzS5LUOD1ZrevXUIu6qaGvNLWsm6bmdVJ57xIA+BjFCAAQM2omJ6hP67rq07ruD8uKS0q1budefbt5t1ZsydW3m3drzbY8vbvwe+3aU7Tf/hmpCWpUK1mN0pN1THqyGtYq+z1FjdKTVSs5XmZW3Q8NABBmFCMAQEyLjwuoVb00taqXpp932n9d9p4ird2RpzXb92jdjj3alJ2v77PztSlnrxZtyNG23IKfjJeSEKe6NRJVOzVRGakJqpMWvF07NVG10xJ+uJ2RmqC0pHjVSIpXzeR4JcUHKFQAEMEoRgAA30pPTdDxqRk6vmlGhesLi0u1OSdfm3NChSk7X5ty8rUjr1A79xRq554ird2+Rzv3FGp3fvFBjxUXMNUIFaUaSfFKS4pTamK8khMCSkqIU3J8XPB26Htywo/fk+KD3xPjAoqPCyg+YIqPM8UFTAlxgeD3QOh7ueXxcab4QCD0PXg7YFLATGairAFACMUIAIADSIwPqFmdVDWrk3rIbYtKSrVrT5F27SnUjrxCZe8tUl5hsXLzi7W7oFh5BcHbuQUlyi0oUm5BsfIKi7U9r1QFxSUqKCpVflGJ8otKVFBcquLS6ps1NhAqSPu+m34sToHQfTMpENh/3Q/7qMy+ZUuXfhxP5e5baL99vWy/5WXW7b/9T/f/cdyy+xxk7HL3pbJZfzpGxbnLP64Dj3HA8fVjKf3p8kqOH9p5v59Jmf0POPahnptyYx9ojAqfm4OOv/8Y+z83Px3jYL8zFT83hx5/3/P0k+fmMPKV/bcRKPPvJlBmmQX0w7p928ftW1fm54PIQjECAKAKJMQFVL9mUpVNE15cUqr84v3LUmFxqUpKnYpK9n13we+lpSopcSouDRaq4pIy25Q6lZSElpc6FZeUqtRJzkmlzsk5J6d9txVaF1pWuv8651xwvdwPYwSXldlXrszYktOP46nMelf2toL3pZ/us28sqdw+5fcvlZxKK9zfhXZ2FY4R3PYn4/8kd/kxfxxDB1p3iMdVqfHLP2/ltkN0+rFYVVCqrHypKrs+tH3gwNsHyqyvcN/KHiuwb/v9C96++3GB8uMfeLymtVN1ZZ/mXj/th0QxAgAgAsXHBVQjLsD04jikHwprBcVKqqislSlWB1nnQo2t4sJXyfHLlcSKiu1hj3E4GbX/Yyz/PB0wnw5QmEPr9v1PhdJQcd83bmnpj8tcmXWlP1lXwb6uon3Ljn3o7UtKKzdeSWlpxccv/en25R+Hcwodp/LZuzevTTEys7Ml/UtSnKRxzrm/lVufJOl5ST0lbZc0yDm3JrTut5KGSSqRNMo59344swIAAEQjM1PcvmvLAByxQLgGNrM4SWMknSOpo6QrzKxjuc2GSdrpnGsr6VFJD4b27SjpckmdJJ0t6YnQeAAAAABQ5cJWjCT1lrTCObfKOVcoabKkgeW2GShpYuj2a5LOsOC70QZKmuycK3DOrZa0IjQeAAAAAFS5cBajJpLWlbm/PrSswm2cc8WSsiXVreS+AAAAAFAlwlmMws7MRphZlpllbd261es4AAAAAKJUOIvRBknNytxvGlpW4TZmFi8pXcFJGCqzr5xzY51zmc65zPr161dhdAAAAAB+Es5iNFtSOzNrZWaJCk6mMLXcNlMlDQ3dvkTSRy44N+JUSZebWZKZtZLUTtJXYcwKAAAAwMfCNl23c67YzEZKel/B6bqfc84tNrP7JGU556ZKelbSJDNbIWmHguVJoe1ekbREUrGkW5xzJeHKCgAAAMDfbN+HV0W7zMxMl5WV5XUMAAAAABHMzOY45zLLL4/qyRcAAAAAoCpQjAAAAAD4HsUIAAAAgO9RjAAAAAD4HsUIAAAAgO9RjAAAAAD4HsUIAAAAgO9RjAAAAAD4HsUIAAAAgO9RjAAAAAD4njnnvM5QJcxsq6S1h9gsXVL2EQx/JPvVk7TtCI6F/R3pzyxSREr+6swRjmNV5ZhHMxavIdEnUv4NHqlIyc9rSNWMxWtI9ImUf4NHKlLyR9prSAvnXP2fLHXO+eZL0tjq2k9SltePNxa+jvRnFilfkZK/OnOE41hVOebRjMVrSPR9Rcq/wWjPz2tI1YzFa0j0fUXKv8Fozx8tryF+u5Tu7WreD0cv2p/7SMlfnTnCcayqHPNoxuI1JPpE+3MfKfl5DamasXgNiT7R/txHSv6oeA2JmUvpIo2ZZTnnMr3OASA68RoC4GjwGgIcPr+dMapOY70OACCq8RoC4GjwGgIcJs4YAQAAAPA9zhgBAAAA8D2KEQAAAADfoxgBAAAA8D2KUTUwszQzm2hmz5jZYK/zAIg+ZtbazJ41s9e8zgIg+pjZBaG/Q6aY2Vle5wEiEcXoCJnZc2a2xcwWlVt+tpktM7MVZnZ3aPFFkl5zzl0vaUC1hwUQkQ7ndcQ5t8o5N8ybpAAi0WG+hrwZ+jvkRkmDvMgLRDqK0ZGbIOnssgvMLE7SGEnnSOoo6Qoz6yipqaR1oc1KqjEjgMg2QZV/HQGA8ibo8F9D7gmtB1AOxegIOec+k7Sj3OLeklaE/s9uoaTJkgZKWq9gOZJ4zgGEHObrCADs53BeQyzoQUn/cc7Nre6sQDTgj/Sq1UQ/nhmSgoWoiaQ3JF1sZk9KetuLYACiRoWvI2ZW18yektTdzH7rTTQAUeBAf4vcKulMSZeY2Y1eBAMiXbzXAfzAOZcn6VqvcwCIXs657Qq+NwAADptzbrSk0V7nACIZZ4yq1gZJzcrcbxpaBgCVxesIgKPBawhwhChGVWu2pHZm1srMEiVdLmmqx5kARBdeRwAcDV5DgCNEMTpCZvaypC8kHWtm681smHOuWNJISe9L+kbSK865xV7mBBC5eB0BcDR4DQGqljnnvM4AAAAAAJ7ijBEAAAAA36MYAQAAAPA9ihEAAAAA36MYAQAAAPA9ihEAAAAA36MYAQAAAPA9ihEAwFNmlhv63tLMrqzisX9X7v7nVTk+ACB2UIwAAJGipaTDKkZmFn+ITfYrRs65Ew4zEwDAJyhGAIBI8TdJJ5nZPDO73czizOxhM5ttZgvM7AZJMrNTzWy6mU2VtCS07E0zm2Nmi81sRGjZ3ySlhMZ7MbRs39kpC429yMwWmtmgMmN/YmavmdlSM3vRzGzfeGa2JJTl79X+7AAAwupQ/6cNAIDqcrekO51z50lSqOBkO+d6mVmSpJlm9t/Qtj0kdXbOrQ7dv845t8PMUiTNNrPXnXN3m9lI51y3Co51kaRukrpKqhfa57PQuu6SOknaKGmmpBPN7BtJF0rq4JxzZpZRtQ8dAOA1zhgBACLVWZKuNrN5kmZJqiupXWjdV2VKkSSNMrP5kr6U1KzMdgfSX9LLzrkS59xmSZ9K6lVm7PXOuVJJ8xS8xC9bUr6kZ83sIkl7jvKxAQAiDMUIABCpTNKtzrluoa9Wzrl9Z4zyftjI7FRJZ0rq55zrKulrSclHcdyCMrdLJMU754ol9Zb0mqTzJL13FOMDACIQxQgAECl2S6pZ5v77km4yswRJMrP2ZpZWwX7pknY65/aYWQdJfcusK9q3fznTJQ0KvY+pvqSTJX11oGBmVkNSunNumqTbFbwEDwAQQ3iPEQAgUiyQVBK6JG6CpH8peBnb3NAECFslXVDBfu9JujH0PqBlCl5Ot89YSQvMbK5zbnCZ5f+W1E/SfElO0l3OuU2hYlWRmpLeMrNkBc9k3XFEjxAAELHMOed1BgAAAADwFJfSAQAAAPA9ihEAAAAA36MYAQAAAPA9ihEAAAAA36MYAQAAAPA9ihEAAAAA36MYAQAAAPA9ihEAAAAA3/t/3cowIFAN4vsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1008x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(iterations), J_history)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss as a function of iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3bdd058ecc5db0eb",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Implement the pseudo-inverse function `pinv`. **Do not use `np.linalg.pinv`**, instead use only direct matrix multiplication as you saw in class (you can calculate the inverse of a matrix using `np.linalg.inv`). (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinv(X, y):\n",
    "    \"\"\"\n",
    "    Calculate the optimal values of the parameters using the pseudoinverse\n",
    "    approach as you saw in class using the *training set*.\n",
    "\n",
    "    Input:\n",
    "    - X: Inputs  (n features over m instances).\n",
    "    - y: True labels (1 value over m instances).\n",
    "\n",
    "    Returns two values:\n",
    "    - theta: The optimal parameters of your model.\n",
    "\n",
    "    ########## DO NOT USE np.linalg.pinv ##############\n",
    "    \"\"\"\n",
    "    \n",
    "    pinv_theta = []\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the pseudoinverse algorithm.                            #\n",
    "    ###########################################################################\n",
    "    pinv_theta = np.linalg.inv(np.transpose(X) @ X) @ np.transpose(X) @ y\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return pinv_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.13346614],\n",
       "       [1.        , 0.18525896],\n",
       "       [1.        , 0.35856574],\n",
       "       ...,\n",
       "       [1.        , 0.12649402],\n",
       "       [1.        , 0.1752988 ],\n",
       "       [1.        , 0.18027888]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-ee89ac06af3087ae",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "theta_pinv = pinv(X_train ,y_train)\n",
    "J_pinv = compute_cost(X_train, y_train, theta_pinv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add the loss value for the theta calculated using the psuedo-inverse to our graph. This is another sanity check as the loss of our model should converge to the psuedo-inverse loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-639b53fc41479335",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(iterations), J_history)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss as a function of iterations')\n",
    "plt.hlines(y = J_pinv, xmin = 0, xmax = len(J_history), color='r',\n",
    "           linewidth = 1, linestyle = 'dashed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5043aa5363cbe5c9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "We can use a better approach for the implementation of `gradient_descent`. Instead of performing 40,000 iterations, we wish to stop when the improvement of the loss value is smaller than `1e-8` from one iteration to the next. Implement the function `efficient_gradient_descent`. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficient_gradient_descent(X, y, theta, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Learn the parameters of your model using the *training set*, but stop \n",
    "    the learning process once the improvement of the loss value is smaller \n",
    "    than 1e-8. This function is very similar to the gradient descent \n",
    "    function you already implemented.\n",
    "\n",
    "    Input:\n",
    "    - X: Inputs  (n features over m instances).\n",
    "    - y: True labels (1 value over m instances).\n",
    "    - theta: The parameters (weights) of the model being learned.\n",
    "    - alpha: The learning rate of your model.\n",
    "    - num_iters: The number of updates performed.\n",
    "\n",
    "    Returns two values:\n",
    "    - theta: The learned parameters of your model.\n",
    "    - J_history: the loss value for every iteration.\n",
    "    \"\"\"\n",
    "    \n",
    "    J_history = [] # Use a python list to save cost in every iteration\n",
    "    theta = theta.copy() # avoid changing the original thetas\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the gradient descent optimization algorithm.            #\n",
    "    ###########################################################################\n",
    "    last_loss = 0\n",
    "    for n in range(num_iters):\n",
    "        theta -= alpha * sum([(np.dot(theta, X[i]) - y[i]) * X[i] for i in range(0,X.shape[0])]) / X.shape[0]\n",
    "        new_loss = compute_cost(X, y, theta)\n",
    "        J_history.append(new_loss)\n",
    "        if new_loss - last_loss <= 1e-8:\n",
    "            break\n",
    "        last_loss = new_loss\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 1.0 \\cdot 10^{-8}$"
      ],
      "text/plain": [
       "1e-08"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6e2524d07523d950",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "The learning rate is another factor that determines the performance of our model in terms of speed and accuracy. Complete the function `find_best_alpha`. Make sure you use the training dataset to learn the parameters (thetas) and use those parameters with the validation dataset to compute the cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_alpha(X_train, y_train, X_val, y_val, iterations):\n",
    "    \"\"\"\n",
    "    Iterate over provided values of alpha and train a model using the \n",
    "    *training* dataset. maintain a python dictionary with alpha as the \n",
    "    key and the loss on the *validation* set as the value.\n",
    "\n",
    "    Input:\n",
    "    - X_train, y_train, X_val, y_val: the training and validation data\n",
    "    - iterations: maximum number of iterations\n",
    "\n",
    "    Returns:\n",
    "    - alpha_dict: A python dictionary - {key (alpha) : value (validation loss)}\n",
    "    \"\"\"\n",
    "    \n",
    "    alphas = [0.00001, 0.00003, 0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 2, 3]\n",
    "    alpha_dict = {}\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the function.                                           #\n",
    "    ###########################################################################\n",
    "    alpha_dict = {alpha: compute_cost(X_val, y_val, efficient_gradient_descent(X_train, y_train, theta, alpha, iterations)[0]) for alpha in alphas}\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return alpha_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a8b088fe7a10910a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-80b77a76ef88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0malpha_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_best_alpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m40\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-70-1263d38335b0>\u001b[0m in \u001b[0;36mfind_best_alpha\u001b[1;34m(X_train, y_train, X_val, y_val, iterations)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# TODO: Implement the function.                                           #\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m###########################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0malpha_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcompute_cost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mefficient_gradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;32min\u001b[0m \u001b[0malphas\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[1;31m###########################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m#                             END OF YOUR CODE                            #\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-70-1263d38335b0>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# TODO: Implement the function.                                           #\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m###########################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0malpha_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mcompute_cost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mefficient_gradient_descent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;32min\u001b[0m \u001b[0malphas\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m     \u001b[1;31m###########################################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m#                             END OF YOUR CODE                            #\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-69-edc989e90e24>\u001b[0m in \u001b[0;36mefficient_gradient_descent\u001b[1;34m(X, y, theta, alpha, num_iters)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mlast_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mtheta\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mnew_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_cost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mJ_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-69-edc989e90e24>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mlast_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mtheta\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mnew_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_cost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mJ_history\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "alpha_dict = find_best_alpha(X_train, y_train, X_val, y_val, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\left\\{ 1.0 \\cdot 10^{-5} : 0.00111988031931092, \\  3.0 \\cdot 10^{-5} : 0.00111988018518559, \\  0.0001 : 0.00111987971574712, \\  0.0003 : 0.00111987837449577, \\  0.001 : 0.00111987368013257, \\  0.003 : 0.00111190972354811, \\  0.01 : 0.00109389657262332, \\  0.03 : 0.00104666325023225, \\  0.1 : 0.000921687132827663, \\  0.3 : 0.000760751540405006, \\  1 : 0.000694199321109249, \\  2 : 0.000693473434443643, \\  3 : 0.000747651350673708\\right\\}$"
      ],
      "text/plain": [
       "{1e-05: 0.0011198803193109177, 3e-05: 0.001119880185185591, 0.0001: 0.00111987\n",
       "9715747124, 0.0003: 0.0011198783744957733, 0.001: 0.001119873680132568, 0.003:\n",
       " 0.0011119097235481059, 0.01: 0.001093896572623321, 0.03: 0.001046663250232249\n",
       "4, 0.1: 0.0009216871328276632, 0.3: 0.0007607515404050055, 1: 0.00069419932110\n",
       "92494, 2: 0.0006934734344436434, 3: 0.0007476513506737082}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5bd93130c022d3e1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Obtain the best learning rate from the dictionary `alpha_dict`. This can be done in a single line using built-in functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4f81cf375ac46b73",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "best_alpha = None\n",
    "###########################################################################\n",
    "#                            START OF YOUR CODE                           #\n",
    "###########################################################################\n",
    "best_alpha = min(alpha_dict, key=alpha_dict.get)\n",
    "###########################################################################\n",
    "#                             END OF YOUR CODE                            #\n",
    "###########################################################################\n",
    "print(best_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.77224477 0.19871568]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1.42091503e+14, 2.95567351e+13]),\n",
       " [0.32035812754452697,\n",
       "  0.37742887797710306,\n",
       "  0.44476098184346374,\n",
       "  0.5241068392047131,\n",
       "  0.6177078132637702,\n",
       "  0.7280181176041621,\n",
       "  0.8581338790105818,\n",
       "  1.0114877024051425,\n",
       "  1.1923608874269263,\n",
       "  1.4055493372258518,\n",
       "  1.6569783485359246,\n",
       "  1.9533426862310346,\n",
       "  2.3028489010941016,\n",
       "  2.714835690011869,\n",
       "  3.2006760643578733,\n",
       "  3.7733890500533955,\n",
       "  4.448743895560401,\n",
       "  5.244881431621115,\n",
       "  6.183675784502705,\n",
       "  7.29039428722164,\n",
       "  8.595390110333224,\n",
       "  10.133844861217773,\n",
       "  11.947889900432713,\n",
       "  14.086497595157438,\n",
       "  16.608162257772623,\n",
       "  19.581037106316245,\n",
       "  23.086351069222925,\n",
       "  27.218932581779875,\n",
       "  32.0916005513228,\n",
       "  37.836278392194494,\n",
       "  44.60968261818089,\n",
       "  52.59531825229676,\n",
       "  62.01090564999445,\n",
       "  73.11167173759627,\n",
       "  86.2001170327263,\n",
       "  101.63119017058206,\n",
       "  119.82521968776557,\n",
       "  141.27580163213008,\n",
       "  166.56702811881874,\n",
       "  196.38525683644318,\n",
       "  231.54219343716989,\n",
       "  272.99218551824777,\n",
       "  321.8633040662031,\n",
       "  379.4824659863951,\n",
       "  447.4174742621738,\n",
       "  527.5131960400832,\n",
       "  621.9486496352904,\n",
       "  733.2887521058836,\n",
       "  864.5621056240459,\n",
       "  1019.3346220375915,\n",
       "  1201.8158307649949,\n",
       "  1416.963186449901,\n",
       "  1670.6277306931722,\n",
       "  1969.7013684858011,\n",
       "  2322.3168936636343,\n",
       "  2738.055364072522,\n",
       "  3228.221299875845,\n",
       "  3806.1340299420963,\n",
       "  4487.50692431283,\n",
       "  5290.855967034925,\n",
       "  6238.023066491156,\n",
       "  7354.748164415775,\n",
       "  8671.3921996331,\n",
       "  10223.73722358266,\n",
       "  12053.986168836416,\n",
       "  14211.880702229615,\n",
       "  16756.08492451762,\n",
       "  19755.745825549493,\n",
       "  23292.4096890633,\n",
       "  27462.198874559603,\n",
       "  32378.467571453995,\n",
       "  38174.836577782495,\n",
       "  45008.87546493959,\n",
       "  53066.331503303496,\n",
       "  62566.237053099074,\n",
       "  73766.80015729915,\n",
       "  86972.49128456422,\n",
       "  102542.24576666491,\n",
       "  120899.29987018785,\n",
       "  142542.60921248503,\n",
       "  168060.50413829312,\n",
       "  198146.58132610732,\n",
       "  233618.6654105761,\n",
       "  275440.92186553957,\n",
       "  324750.1913592896,\n",
       "  382886.7544351327,\n",
       "  451430.91062301764,\n",
       "  532245.7741716111,\n",
       "  627528.0896080466,\n",
       "  739867.7567019184,\n",
       "  872318.4304869934,\n",
       "  1028480.3071828157,\n",
       "  1212598.2199812522,\n",
       "  1429676.7665829845,\n",
       "  1685616.61905449,\n",
       "  1987374.6170415648,\n",
       "  2343153.1898138523,\n",
       "  2762622.965055663,\n",
       "  3257186.011300617,\n",
       "  3840285.3597745826,\n",
       "  4527770.851693146,\n",
       "  5338329.48962665,\n",
       "  6293993.900554084,\n",
       "  7420740.665766921,\n",
       "  8749197.015924495,\n",
       "  10315472.68327715,\n",
       "  12162142.137352183,\n",
       "  14339401.074031243,\n",
       "  16906431.65431472,\n",
       "  19933010.29086857,\n",
       "  23501405.2681802,\n",
       "  27708611.899926875,\n",
       "  32668990.17485807,\n",
       "  38517372.04704134,\n",
       "  45412727.74586424,\n",
       "  53542485.4452908,\n",
       "  63127627.464229204,\n",
       "  74428695.2930557,\n",
       "  87752873.41641453,\n",
       "  103462337.1855714,\n",
       "  121984099.55111194,\n",
       "  143821615.75635147,\n",
       "  169568470.68680355,\n",
       "  199924510.90015942,\n",
       "  235714870.6868667,\n",
       "  277912397.57591224,\n",
       "  327664099.5738592,\n",
       "  386322319.24786144,\n",
       "  455481496.9372154,\n",
       "  537021506.2346523,\n",
       "  633158757.1663604,\n",
       "  746506437.2089349,\n",
       "  880145548.1188371,\n",
       "  1037708647.2711041,\n",
       "  1223478593.8138306,\n",
       "  1442504957.680801,\n",
       "  1700741283.9033947,\n",
       "  2005206912.7444413,\n",
       "  2364177787.3457274,\n",
       "  2787411399.5176744,\n",
       "  3286412026.980462,\n",
       "  3874743429.593404,\n",
       "  4568397549.849786,\n",
       "  5386229190.182588,\n",
       "  6350468535.9585905,\n",
       "  7487325397.475776,\n",
       "  8827701658.238129,\n",
       "  10408031227.06686,\n",
       "  12271270399.304504,\n",
       "  14468065466.997356,\n",
       "  17058129402.525305,\n",
       "  20111864942.586033,\n",
       "  23712278292.73659,\n",
       "  27957235360.2447,\n",
       "  32962121963.365143,\n",
       "  38862980196.93772,\n",
       "  45820206346.924324,\n",
       "  54022910717.91011,\n",
       "  63694058046.68892,\n",
       "  75096528049.15057,\n",
       "  88540261031.15367,\n",
       "  104390682584.29462,\n",
       "  123078636593.1265,\n",
       "  145112096304.58722,\n",
       "  171089972055.77298,\n",
       "  201718390689.6821,\n",
       "  237829889478.10062,\n",
       "  280406045937.14185,\n",
       "  330604159025.4524,\n",
       "  389788706568.8136,\n",
       "  459568434416.7595,\n",
       "  541840085012.30457,\n",
       "  638839954509.0729,\n",
       "  753204679300.3618,\n",
       "  888042904850.9299,\n",
       "  1047019784272.6287,\n",
       "  1234456604177.5708,\n",
       "  1455448245059.1804,\n",
       "  1716001669822.0847,\n",
       "  2023199203935.9329,\n",
       "  2385391046447.053,\n",
       "  2812422243595.517,\n",
       "  3315900295749.9595,\n",
       "  3909510670468.0547,\n",
       "  4609388799263.262,\n",
       "  5434558668175.763,\n",
       "  6407449925441.672,\n",
       "  7554507560483.439,\n",
       "  8906910728296.025,\n",
       "  10501420256183.236,\n",
       "  12381377871848.143,\n",
       "  14597884311325.508,\n",
       "  17211188332503.857,\n",
       "  20292324387395.605,\n",
       "  23925043471168.5,\n",
       "  28208089628699.973,\n",
       "  33257884001902.527,\n",
       "  39211689371326.41,\n",
       "  46231341214362.82,\n",
       "  54507646692564.15,\n",
       "  64265571145675.85,\n",
       "  75770353065530.62,\n",
       "  89334713771981.2,\n",
       "  105327357756910.16,\n",
       "  124182994758293.64,\n",
       "  146414155975458.97,\n",
       "  172625125620263.72,\n",
       "  203528366480825.2,\n",
       "  239963885984654.66,\n",
       "  282922069156323.1,\n",
       "  333570599123128.9,\n",
       "  393286196905791.56,\n",
       "  463692043253861.25,\n",
       "  546701899706374.0,\n",
       "  644572128185615.4,\n",
       "  759963023096253.0,\n",
       "  896011123068325.0,\n",
       "  1056414467891552.6,\n",
       "  1245533118104334.8,\n",
       "  1468507669522805.0,\n",
       "  1731398984180243.2,\n",
       "  2041352935795498.0,\n",
       "  2406794647887701.0,\n",
       "  2837657504257353.0,\n",
       "  3345653156798755.0,\n",
       "  3944589869918855.5,\n",
       "  4650747854795763.0,\n",
       "  5483321795714264.0,\n",
       "  6464942597212828.0,\n",
       "  7622292534045820.0,\n",
       "  8986830524935496.0,\n",
       "  1.0595647244336696e+16,\n",
       "  1.2492473315808068e+16,\n",
       "  1.472886799148185e+16,\n",
       "  1.736562062822439e+16,\n",
       "  2.0474403055127036e+16,\n",
       "  2.413971774683019e+16,\n",
       "  2.8461194757549492e+16,\n",
       "  3.3556299850839828e+16,\n",
       "  3.956352743697245e+16,\n",
       "  4.6646165108012616e+16,\n",
       "  5.499673209751317e+16,\n",
       "  6.4842212310532136e+16,\n",
       "  7.64502241672982e+16,\n",
       "  9.013629496847702e+16,\n",
       "  1.0627243751260056e+17,\n",
       "  1.2529726209425534e+17,\n",
       "  1.4772789874565645e+17,\n",
       "  1.7417405379050816e+17,\n",
       "  2.0535458279311062e+17,\n",
       "  2.4211703038649254e+17,\n",
       "  2.854606681080455e+17,\n",
       "  3.365636564541368e+17,\n",
       "  3.968150694683482e+17,\n",
       "  4.678526523514668e+17,\n",
       "  5.5160733841472954e+17,\n",
       "  6.503557354301678e+17,\n",
       "  7.667820080538717e+17,\n",
       "  9.040508384019338e+17,\n",
       "  1.0658934479821559e+18,\n",
       "  1.2567090192179976e+18,\n",
       "  1.4816842733892872e+18,\n",
       "  1.7469344553406044e+18,\n",
       "  2.0596695571825536e+18,\n",
       "  2.42839029925571e+18,\n",
       "  2.8631191954819323e+18,\n",
       "  3.3756729838896154e+18,\n",
       "  3.9799838274787927e+18,\n",
       "  4.692478016262389e+18,\n",
       "  5.532522464307105e+18,\n",
       "  6.522951138393203e+18,\n",
       "  7.69068572759837e+18,\n",
       "  9.067467424760643e+18,\n",
       "  1.0690719710993846e+19,\n",
       "  1.2604565595343116e+19,\n",
       "  1.4861026960039752e+19,\n",
       "  1.7521438611785495e+19,\n",
       "  2.065811547560439e+19,\n",
       "  2.4356318248682426e+19,\n",
       "  2.871657094431748e+19,\n",
       "  3.3857393321120956e+19,\n",
       "  3.991852246996485e+19,\n",
       "  4.7064711127393395e+19,\n",
       "  5.549020596069439e+19,\n",
       "  6.5424027552739025e+19,\n",
       "  7.713619560636477e+19,\n",
       "  9.094506858091348e+19,\n",
       "  1.0722599726585904e+20,\n",
       "  1.2642152751173208e+20,\n",
       "  1.4905342944745213e+20,\n",
       "  1.7573688016057854e+20,\n",
       "  2.0719718535198936e+20,\n",
       "  2.4428949449062407e+20,\n",
       "  2.880220453627479e+20,\n",
       "  3.395835698457551e+20,\n",
       "  4.003756058462486e+20,\n",
       "  4.7205059370087966e+20,\n",
       "  5.56556792570725e+20,\n",
       "  6.561912377402902e+20,\n",
       "  7.736621782985592e+20,\n",
       "  9.121626923744074e+20,\n",
       "  1.0754574809247643e+21,\n",
       "  1.2679851992919171e+21,\n",
       "  1.4949791080916336e+21,\n",
       "  1.7626093229467553e+21,\n",
       "  2.0781505296784449e+21,\n",
       "  2.4501797237647585e+21,\n",
       "  2.8888093489920433e+21,\n",
       "  3.40596217244066e+21,\n",
       "  4.0156953674164727e+21,\n",
       "  4.734582613504055e+21,\n",
       "  5.582164599929947e+21,\n",
       "  6.58148017775294e+21,\n",
       "  7.759692598584278e+21,\n",
       "  9.14882786216655e+21,\n",
       "  1.0786645242470956e+22,\n",
       "  1.2717663654824445e+22,\n",
       "  1.4994371762632622e+22,\n",
       "  1.767865471664278e+22,\n",
       "  2.0843476308166166e+22,\n",
       "  2.4574862260310395e+22,\n",
       "  2.897423856675167e+22,\n",
       "  3.4161188438434222e+22,\n",
       "  4.027670279713023e+22,\n",
       "  4.748701267029828e+22,\n",
       "  5.598810765884576e+22,\n",
       "  6.601106329813728e+22,\n",
       "  7.782832211980002e+22,\n",
       "  9.176109914524367e+22,\n",
       "  1.0818811310595044e+23,\n",
       "  1.2755588072129958e+23,\n",
       "  1.5039085385149704e+23,\n",
       "  1.7731372943596275e+23,\n",
       "  2.090563211878223e+23,\n",
       "  2.4648145164850375e+23,\n",
       "  2.9060640530535154e+23,\n",
       "  3.426305802715346e+23,\n",
       "  4.0396809015224045e+23,\n",
       "  4.7628620227626773e+23,\n",
       "  5.615506571156736e+23,\n",
       "  6.620791007591194e+23,\n",
       "  7.806040828329088e+23,\n",
       "  9.203473322700628e+23,\n",
       "  1.0851073298804941e+24,\n",
       "  1.2793625581074067e+24,\n",
       "  1.5083932344898883e+24,\n",
       "  1.778424837772908e+24,\n",
       "  2.0967973279707422e+24,\n",
       "  2.4721646600994327e+24,\n",
       "  2.914730014731202e+24,\n",
       "  3.4365231393742866e+24,\n",
       "  4.051727339330954e+24,\n",
       "  4.777065006252453e+24,\n",
       "  5.632252163772214e+24,\n",
       "  6.640534385610624e+24,\n",
       "  7.829318653400542e+24,\n",
       "  9.2309183293009e+24,\n",
       "  1.0883431493136892e+25,\n",
       "  1.2831776518899443e+25,\n",
       "  1.5128913039496022e+25,\n",
       "  1.7837281487837608e+25,\n",
       "  2.1030500343661743e+25,\n",
       "  2.479536722041053e+25,\n",
       "  2.923421818541245e+25,\n",
       "  3.4467709444077246e+25,\n",
       "  4.0638096999431705e+25,\n",
       "  4.791310343423412e+25,\n",
       "  5.649047692198145e+25,\n",
       "  6.660336638917882e+25,\n",
       "  7.85266589357654e+25,\n",
       "  9.258445177653963e+25,\n",
       "  1.0915886180480417e+26,\n",
       "  1.2870041223854651e+26,\n",
       "  1.5174027867742689e+26,\n",
       "  1.7890472744116864e+26,\n",
       "  2.109321386501467e+26,\n",
       "  2.486930767671074e+26,\n",
       "  2.93213954154569e+26,\n",
       "  3.457049308673367e+26,\n",
       "  4.07592809048197e+26,\n",
       "  4.805598160575624e+26,\n",
       "  5.665893305344662e+26,\n",
       "  6.680197943080718e+26,\n",
       "  7.876082755855017e+26,\n",
       "  9.286054111814082e+26,\n",
       "  1.0948437648580518e+27,\n",
       "  1.2908420035195972e+27,\n",
       "  1.5219277229628692e+27,\n",
       "  1.7943822618162583e+27,\n",
       "  2.1156114399785892e+27,\n",
       "  2.4943468625453106e+27,\n",
       "  2.940883261036126e+27,\n",
       "  3.467358323299545e+27,\n",
       "  4.088082618389513e+27,\n",
       "  4.8199285843854835e+27,\n",
       "  5.682789152565768e+27,\n",
       "  6.700118474190192e+27,\n",
       "  7.899569447850454e+27,\n",
       "  9.313745376563073e+27,\n",
       "  1.0981086186039345e+28])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = np.random.random(size=2)\n",
    "print(theta)\n",
    "efficient_gradient_descent(X_train, y_train, theta, best_alpha, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d16367ecb7183996",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Pick the best three alpha values you just calculated and provide **one** graph with three lines indicating the training loss as a function of iterations (Use 10,000 iterations). Note you are required to provide general code for this purpose (no hard-coding). Make sure the visualization is clear and informative. (5 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-448638e817503ca3",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-68-300ba390302e>:7: UserWarning: Data has no positive values, and therefore cannot be log-scaled.\n",
      "  plt.xscale('log')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###########################################################################\n",
    "#                            START OF YOUR CODE                           #\n",
    "###########################################################################\n",
    "theta = np.random.random(size=2)\n",
    "history = efficient_gradient_descent(X_train, y_train, theta, best_alpha, 10000)[1]\n",
    "plt.plot(np.arange(len(history)), history)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss as a function of iterations')\n",
    "plt.show()\n",
    "###########################################################################\n",
    "#                             END OF YOUR CODE                            #\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b73893d236bff1d5",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "This is yet another sanity check. This function plots the regression lines of your model and the model based on the pseudoinverse calculation. Both models should exhibit the same trend through the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c7ee7d8763464371",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "plt.plot(X_train[:,1], y_train, 'ro', ms=1, mec='k')\n",
    "plt.ylabel('Price in USD')\n",
    "plt.xlabel('sq.ft')\n",
    "plt.plot(X_train[:, 1], np.dot(X_train, theta), 'o')\n",
    "plt.plot(X_train[:, 1], np.dot(X_train, theta_pinv), '-')\n",
    "\n",
    "plt.legend(['Training data', 'Linear regression', 'Best theta']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e77c602466fab37d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Part 2: Multivariate Linear Regression (30 points)\n",
    "\n",
    "In most cases, you will deal with databases that have more than one feature. It can be as little as two features and up to thousands of features. In those cases, we use a multiple linear regression model. The regression equation is almost the same as the simple linear regression equation:\n",
    "\n",
    "$$\n",
    "\\hat{y} = h_\\theta(\\vec{x}) = \\theta^T \\vec{x} = \\theta_0 + \\theta_1 x_1 + ... + \\theta_n x_n\n",
    "$$\n",
    "\n",
    "\n",
    "If you wrote vectorized code, this part should be straightforward. If your code is not vectorized, you should go back and edit your functions such that they support both multivariate and single variable regression. **Your code should not check the dimensionality of the input before running**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-15626dda8db26550",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Read comma separated data\n",
    "df = pd.read_csv('data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2dc0f4dc3491520c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "Like in the single variable case, we need to create a numpy array from the dataframe. Before doing so, we should notice that some of the features are clearly irrelevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a87b4027bd3bda4b",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop(columns=['price', 'id', 'date']).values\n",
    "y = df['price'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1aa12f54513b1efa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Use the **same** `preprocess` function you implemented previously. Notice that proper vectorized implementation should work regardless of the dimensionality of the input. You might want to check that your code in the previous parts still works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f40a9df530db9399",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "X, y = preprocess(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and validation split\n",
    "np.random.seed(42)\n",
    "indices = np.random.permutation(X.shape[0])\n",
    "idx_train, idx_val = indices[:int(0.8*X.shape[0])], indices[int(0.8*X.shape[0]):]\n",
    "X_train, X_val = X[idx_train,:], X[idx_val,:]\n",
    "y_train, y_val = y[idx_train], y[idx_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 3D visualization, we can still observe trends in the data. Visualizing additional dimensions requires advanced techniques we will learn later in the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0c68216a26a9b5af",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "fig = plt.figure(figsize=(5,5))\n",
    "ax = p3.Axes3D(fig)\n",
    "xx = X_train[:, 1][:1000]\n",
    "yy = X_train[:, 2][:1000]\n",
    "zz = y_train[:1000]\n",
    "ax.scatter(xx, yy, zz, marker='o')\n",
    "ax.set_xlabel('bathrooms')\n",
    "ax.set_ylabel('sqft_living')\n",
    "ax.set_zlabel('price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-70fcd47d69caea00",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Use the bias trick again (add a column of ones as the zeroth column in the both the training and validation datasets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-2985911f4b7af3e1",
     "locked": false,
     "points": 0,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "#                            START OF YOUR CODE                           #\n",
    "###########################################################################\n",
    "pass\n",
    "###########################################################################\n",
    "#                             END OF YOUR CODE                            #\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2b89288ff61c80ac",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Make sure the functions `compute_cost` (10 points), `gradient_descent` (15 points), and `pinv` (5 points) work on the multi-dimensional dataset. If you make any changes, make sure your code still works on the single variable regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-81ab741781b2f6ec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "shape = X_train.shape[1]\n",
    "theta = np.ones(shape)\n",
    "J = compute_cost(X_train, y_train, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6f25fb05bd6c648a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "shape = X_train.shape[1]\n",
    "theta = np.random.random(shape)\n",
    "iterations = 40000\n",
    "theta, J_history = gradient_descent(X_train ,y_train, theta, best_alpha, iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-827d1de1293be51f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "theta_pinv = pinv(X_train ,y_train)\n",
    "J_pinv = compute_cost(X_train, y_train, theta_pinv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use visualization to make sure the code works well. Notice we use logarithmic scale for the number of iterations, since gradient descent converges after ~500 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4fa207b72d2445c2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(iterations), J_history)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss as a function of iterations - multivariate linear regression')\n",
    "plt.hlines(y = J_pinv, xmin = 0, xmax = len(J_history), color='r',\n",
    "           linewidth = 1, linestyle = 'dashed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-cad652570cee3629",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Part 3: Find best features for regression (10 points)\n",
    "\n",
    "Adding additional features to our regression model makes it more complicated but does not necessarily improves performance.\n",
    "Use forward and backward selection and find 4 features that best minimizes the loss. First, we will reload the dataset as a dataframe in order to access the feature names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['price', 'id', 'date']\n",
    "all_features = df.drop(columns=columns_to_drop)\n",
    "all_features.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Feature Selection\n",
    "\n",
    "Complete the function `forward_selection`. Train the model using a single feature at a time, and choose the best feature using the validation dataset. Next, check which feature performs best when added to the feature you previously chose. Repeat this process until you reach 4 features + bias. You are free to use any arguments you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_selection():\n",
    "    \"\"\"\n",
    "    Train the model using the training set using a single feature. \n",
    "    Choose the best feature according to the validation set. Next, \n",
    "    check which feature performs best when added to the feature\n",
    "    you previously chose. Repeat this process until you reach 4 \n",
    "    features and the bias. Don't forget the bias trick.\n",
    "\n",
    "    Returns:\n",
    "    - The names of the best features using forward selection.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    best_features = None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the function.                                           #\n",
    "    ###########################################################################\n",
    "    pass\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return best_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Feature Selection\n",
    "\n",
    "Complete the function `backward_selection`. Train the model with all but one of the features at a time and remove the worst feature (the feature that its absence yields the best loss value using the validation dataset). Next, remove an additional feature along with the feature you previously removed. Repeat this process until you reach 4 features + bias. You are free to use any arguments you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_selection():\n",
    "    \"\"\"\n",
    "    Train the model using the training set using all but one of the \n",
    "    features at a time. Remove the worst feature according to the \n",
    "    validation set. Next, remove an additional feature along with the \n",
    "    feature you previously removed. Repeat this process until you \n",
    "    reach 4 features and the bias. Don't forget the bias trick.\n",
    "\n",
    "    Returns:\n",
    "    - The names of the best features using backward selection.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    best_features = None\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the function.                                           #\n",
    "    ###########################################################################\n",
    "    pass\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backward_selection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give an explanations to the results. Do they make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use this Markdown cell for your answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Adaptive Learning Rate (10 points)\n",
    "\n",
    "So far, we kept the learning rate alpha constant during training. However, changing alpha during training might improve convergence in terms of the global minimum found and running time. Implement the adaptive learning rate method based on the gradient descent algorithm above. \n",
    "\n",
    "**Your task is to find proper hyper-parameter values for the adaptive technique and compare this technique to the constant learning rate. Use clear visualizations of the validation loss and the learning rate as a function of the iteration**. \n",
    "\n",
    "Time based decay: this method reduces the learning rate every iteration according to the following formula:\n",
    "\n",
    "$$\\alpha = \\frac{\\alpha_0}{1 + D \\cdot t}$$\n",
    "\n",
    "Where $\\alpha_0$ is the original learning rate, $D$ is a decay factor and $t$ is the current iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here ###"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
