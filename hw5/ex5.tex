
\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}

\newcommand\tab[1][1cm]{\hspace*{#1}}

\linespread{1.3}


\title{Machine Learning from Data -IDC\\HW5â€“Theory+ SVM}
\author{227367455 and 323081950}

\begin{document}

\maketitle

\section*{\centering{Question 1)}}

\subsection*{a)}

Let $K,L$ be two kernels (operating on the same space)and let $ \alpha, \beta $ be two positive scalars \\
\tab Prove that $\alpha K + \beta L $ is a kernel.

As $K(x,y) = \langle\varphi_{1}(x), \varphi_{1}(y)\rangle$, $L(x,y) = \langle\varphi_{2}(x)\, \varphi_{2}(y)\rangle$ (inner product,) \\
		then $\alpha K(x,y) + \beta L(x,y) = \alpha \cdot \langle \varphi_{1}(x),\varphi_{1}(y)\rangle + \beta \cdot \langle\varphi_{2}(x\,\varphi_{2}(y)\rangle $ \\
		$ = \langle \alpha \cdot \varphi_{1}(x),\varphi_{1}(y)\rangle + \langle\beta \cdot\varphi_{2}(x),\varphi_{2}(y)\rangle $ by linearity of inner product\\
		
\subsection*{b)}

Provide (two different) examples of non-zero kernels $K,L$ (operating on the same space), so that:

\paragraph{\tab i.}

$K - L$ is a kernel

\paragraph{\tab ii.}

$K - L$ is not a kernel

Suppose we are working over the space $\mathbb{R}$.
		\begin{enumerate}
			\item Let $K(x,y) = \varphi_1(x)\cdot\varphi_1(y)$ and $K(x,y) = \varphi_2(x)\cdot\varphi_2(y)$ \\
			Define $\varphi_1(x) = (2x, -4x^2)$ and $\varphi_2(x) = (x, x^2) $
			Notice that $ K(x,y) - L(x,y) = \varphi_1(x)\cdot\varphi_1(y) - \varphi_2(x)\cdot\varphi_2(y) $ \\
			$ = (2x, -4x^2)\cdot(2y, -4y^2) - (x, x^2)\cdot(y,y^2) $ \\
			$ = 2x \cdot 2y + (-4x^2) \cdot (-4y^2) - x \cdot y - x^2 \cdot y^2 $ \\
			$ = 4(x \cdot y) + 16(x^2 \cdot y^2) - x \cdot y - x^2 \cdot y^2 $ \\
			$ = 3(x \cdot y) + 15(x^2 \cdot y^2) $ \\
			$ = 3x \cdot 3y + 15x^2 \cdot 15y^2 $ \\
			This is a kernel with the mapping $ \varphi(x) = (3x, 15x) $.
			\item Let $K(x,y) = \varphi_1(x)\cdot\varphi_1(y)$ and $K(x,y) = \varphi_2(x)\cdot\varphi_2(y)$ \\
			Define $\varphi_1(x) = (x^4, x^2)$ and $\varphi_2(x) = (2x^4, -4x^2) $
			Notice that $ K(x,y) - L(x,y) = \varphi_1(x)\cdot\varphi_1(y) - \varphi_2(x)\cdot\varphi_2(y) $ \\
			$ = (x^4, x^2)\cdot(y^4,y^2) - (2x^4, -4x^2)\cdot(2y^4, -4y^2) $ \\
			$ = x^4 \cdot y^4 + x^2 \cdot y^2 - 2x^4 \cdot 2y^4 - (-4x^2) \cdot (-4y^2) $ \\
			$ = x^4 \cdot y^4 + x^2 \cdot y^2 - 2(x^4 \cdot y^4) - 4(x^2 \cdot y^2) $ \\
			$ = -(x^4 \cdot y^4) - 3(x^2 \cdot y^2) $ \\
			Notice that this is not a kernel. Proof: \\
			ATC that this is a kernel. That means there exists a mapping $\varphi(x)$ \\
			s.t. $ -(x^4 \cdot y^4) - 3(x^2 \cdot y^2) = \varphi(x) \cdot \varphi(y) $ \\
			Notice that the left side is always negative. \\
			However, by the basic properties of an inner product, then the right side must  $ \geq 0 $. \\
			This means we have a inner product (that is always positive) equal a negative number, which is a contradiction!
		\end{enumerate}

\newpage

\section*{\centering{Question 2)}}


\tab Use Lagrange Multipliers to find the maximum and minimum values of the function subject to the given constraints: \\
\tab Function: $f(x,y,z,) = x^2 + y^2 + z^2$.  \\
\tab Constraint: $g(x,y,z) = \frac{x^2}{\alpha^2} + \frac{y^2}{\beta^2} + \frac{z^2}{\beta^2} = 1$, where $ \alpha > \beta > 0$ 


$$
\bigtriangledown f = 
\begin{bmatrix}
\frac{\partial f}{\partial x} \\
\frac{\partial f}{\partial y} \\
\frac{\partial f}{\partial z} 
\end{bmatrix} = 2
\begin{bmatrix}
x \\
y \\
z
\end{bmatrix}
 $$
 
$$
\bigtriangledown g = 
\begin{bmatrix}
\frac{\partial g}{\partial x} \\
\frac{\partial g}{\partial y} \\
\frac{\partial g}{\partial z} 
\end{bmatrix} = 2
\begin{bmatrix}
\frac{x}{\alpha^2} \\
\frac{y}{\beta^2} \\
\frac{z}{\beta^2}
\end{bmatrix}
 $$

$$
\bigtriangledown f  = \lambda\bigtriangledown g
$$

$$
\iff \begin{bmatrix}
x \\
y \\
z
\end{bmatrix}
- \lambda
\begin{bmatrix}
\frac{x}{\alpha^2} \\
\frac{y}{\beta^2} \\
\frac{z}{\beta^2}
\end{bmatrix}
= 0
 $$
 
$$ \iff
\begin{bmatrix}
x(1 - \frac{\lambda}{\alpha^2} ) \\
y(1 - \frac{\lambda}{\beta^2} ) \\
z(1 - \frac{\lambda}{\beta^2} ) \\
\end{bmatrix}
= 0
 $$ \\

Notice that if $ x \neq 0 \Rightarrow \lambda = \alpha^2$ and if $ y \neq 0 \bigvee  z \neq 0 \Rightarrow \lambda = \beta^2$. Therefore:  $\alpha = \beta$ contradicting our constraint.

Consider $x \neq 0$ :

$$ \frac{x^2}{\alpha^2} + \frac{y^2}{\beta^2} + \frac{z^2}{\beta^2} = \frac{x^2}{\alpha^2} = 1 \iff x = \pm \alpha$$

Consider $x = 0$ :

$$ \frac{x^2}{\alpha^2} + \frac{y^2}{\beta^2} + \frac{z^2}{\beta^2} = 
\frac{y^2}{\beta^2} + \frac{z^2}{\beta^2} = 1$$

$$ y^2 + z^2 = \beta^2 $$ \\ 

Notice: $\alpha > \beta > 0 \Rightarrow \alpha^2 > \beta^2$ \\

Therefore we can define the maxima $f(\pm \alpha,0,0) = \alpha^2 $\\

And minima $f(0,x,z) = y^2 + z^2 = \beta^2 $


\section*{\centering{Question 3)}}


\tab Let $X = \mathbb{R}^2$. Let $ C = H =  \lbrace h(a,b,c) = \lbrace(x,y,z) \mid |x| \leq a, |y| \leq b, |z| \leq c \rbrace \mid  a,b,c \in \mathbb{R}^+\rbrace $ the set of all origin centered boxes. \\
\tab Describe the polynomial sample complexity algorithm $L$ that learns $C$ using $H$. State the time complexity and sample complexity of your suggested algorithm. Prove all your steps.


\end{document}
