{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4: Logistic Regression, Bayes and EM\n",
    "\n",
    "In this assignment you will implement several algorithms as learned in class.\n",
    "\n",
    "## Read the following instructions carefully:\n",
    "\n",
    "1. This jupyter notebook contains all the step by step instructions needed for this exercise.\n",
    "2. Write **efficient vectorized** code whenever possible. Some calculations in this exercise take several minutes when implemented efficiently, and might take much longer otherwise. Unnecessary loops will result in point deduction.\n",
    "3. You are responsible for the correctness of your code and should add as many tests as you see fit. Tests will not be graded nor checked.\n",
    "4. Write your functions in this notebook only. **Do not create Python modules and import them**.\n",
    "5. You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/) and [numpy](https://www.numpy.org/devdocs/reference/) and pandas. \n",
    "6. Your code must run without errors. During the environment setup, you were given a specific version of `numpy` to install (1.15.4). Changes of the configuration we provided are at your own risk. Any code that cannot run will not be graded.\n",
    "7. Write your own code. Cheating will not be tolerated.\n",
    "8. Submission includes this notebook only with the exercise number and your ID as the filename. For example: `hw4_123456789_987654321.ipynb` if you submitted in pairs and `hw4_123456789.ipynb` if you submitted the exercise alone.\n",
    "9. Answers to qualitative questions should be written in **markdown** cells (with $\\LaTeX$ support). Answers that will be written in commented code blocks will not be checked.\n",
    "\n",
    "## In this exercise you will perform the following:\n",
    "1. Implement Logistic Regression algorithm.\n",
    "1. Implement EM algorithm.\n",
    "1. Implement Navie Bayes algorithm that uses EM for calculating the likelihood.\n",
    "1. Visualize your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I have read and understood the instructions: *** YOUR ID HERE ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# make matplotlib figures appear inline in the notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# Make the notebook automatically reload external python modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for ploting the decision boundaries of a model\n",
    "# You will use it later\n",
    "def plot_decision_regions(X, y, classifier, resolution=0.01):\n",
    "\n",
    "    # setup marker generator and color map\n",
    "    markers = ('.', '.')\n",
    "    colors = ('blue', 'red')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], \n",
    "                    y=X[y == cl, 1],\n",
    "                    alpha=0.8, \n",
    "                    c=colors[idx],\n",
    "                    marker=markers[idx], \n",
    "                    label=cl, \n",
    "                    edgecolor='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pd.read_csv('training_set.csv')\n",
    "test_set = pd.read_csv('test_set.csv')\n",
    "X_training, y_training = training_set[['x1', 'x2']].values, training_set['y'].values\n",
    "X_test, y_test = test_set[['x1', 'x2']].values, test_set['y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.38578869,  9.13146361],\n",
       "       [-2.23690338,  5.56117803],\n",
       "       [-2.3625238 ,  5.2159729 ],\n",
       "       ...,\n",
       "       [ 7.42420048,  1.77493117],\n",
       "       [ 6.67073987,  0.33570079],\n",
       "       [ 4.16906648,  1.64874303]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Visualizing the data\n",
    "(2 points each - 8 points total)\n",
    "\n",
    "Plot the following graphs for the training set:\n",
    "\n",
    "For the first feature only:\n",
    "1. For the first 1000 data points plot a histogram for each class on the same graph (use bins=20, alpha=0.5).\n",
    "1. For all the data points plot a histogram for each class on the same graph (use bins=40, alpha=0.5).\n",
    "\n",
    "For both features:\n",
    "1. For the first 1000 data points plot a scatter plot where each class has different color\n",
    "1. For all the data points plot a scatter plot where each class has different color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Your code here ####\n",
    "fig = plt.figure()\n",
    "gs = fig.add_gridspec(2, 2, hspace=0, wspace=0)\n",
    "(ax1, ax2), (ax3, ax4) = gs.subplots(sharex='col', sharey='row')\n",
    "\n",
    "ax1.hist(X_training[:,1][:1000], bins=20, alpha = 0.5)\n",
    "ax1.plot()\n",
    "\n",
    "ax2.hist(X_training, bins=40, alpha = 0.5)\n",
    "ax2.plot()\n",
    "\n",
    "\n",
    "ax3.plot(X_training[:1000], y_training[:1000], 'o', alpha = 0.5)\n",
    "ax3.plot()\n",
    "\n",
    "for i in range(X_training.shape[1]):\n",
    "    ax4.plot(X_training[:,i], y_training, 'o', alpha = 0.5)\n",
    "ax4.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "(15 Points)\n",
    "\n",
    "Implement the Logistic Regression algorithm that uses gradient descent for finding the optimal theta vector. \n",
    "\n",
    "Where:\n",
    "$$\n",
    "h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^T x}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "J(\\theta)=\\frac{1}{m} \\sum_{d=1}^{m} - y^{(d)}ln(h_\\theta(x^{(d)}) - (1 - y^{(d)})ln(1 - h_\\theta(x^{(d)})\n",
    "$$\n",
    "\n",
    "Your class should contain the following functions:\n",
    "1. fit - the learning function\n",
    "1. predict - the function for predicting an instance after the fit function was executed\n",
    "\n",
    "\\* You can add more functions if you think this is necessary\n",
    "\n",
    "Your model should also store a list of the costs that you've calculated in each iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(X_test, y_test, logistic_regression_classifier):\n",
    "    X_test = bias_trick(X_test)\n",
    "    predictions = [logistic_regression_classifier.predict(x) for x in X_test]\n",
    "    return (predictions == y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bias_trick(data):\n",
    "    \"\"\"\n",
    "    Bias trick a data st by adding a column of ones as the zeroeth column\n",
    "\n",
    "    Input:\n",
    "    - data: Data model as an np array.\n",
    "\n",
    "    Returns:\n",
    "    - data whose 0th column is all 1's. The shape of the return type is (data.shape[0], data.shape[1] + 1)\n",
    "    \"\"\"\n",
    "    return np.c_[np.ones(data.shape[0], dtype=data.dtype), data]\n",
    "\n",
    "\n",
    "class LogisticRegressionGD(object):\n",
    "    \"\"\"\n",
    "    Logistic Regression Classifier using gradient descent.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    eta : float\n",
    "      Learning rate (between 0.0 and 1.0)\n",
    "    n_iter : int\n",
    "      Passes over the training dataset.\n",
    "    eps : float\n",
    "      minimal change in the cost to declare convergence\n",
    "    random_state : int\n",
    "      Random number generator seed for random weight\n",
    "      initialization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, eta=0.00005, n_iter=10000, eps=0.000001, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.eps = eps\n",
    "        self._J_history = []\n",
    "        self.random_state = random_state\n",
    "        self._theta = None\n",
    "        \n",
    "    def gradient(self, X, y, h):\n",
    "        gradient = (X.T @ (h - y)).mean()\n",
    "        return gradient\n",
    "    \n",
    "    def compute_cost(self, h, y):\n",
    "        cost = (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "        return cost\n",
    "    \n",
    "    def sigmoid_func(self, X):\n",
    "        compute_func = 1 / (1 + np.power(np.e, - X @ self._theta))\n",
    "        return compute_func\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\" \n",
    "        Fit training data (the learning phase).\n",
    "        Updating the theta vector in each iteration using gradient descent.\n",
    "        Store the theta vector in an attribute of the LogisticRegressionGD object.\n",
    "        Stop the function when the difference between the previous cost and the current is less than eps\n",
    "        or when you reach n_iter.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like}, shape = [n_examples, n_features]\n",
    "          Training vectors, where n_examples is the number of examples and\n",
    "          n_features is the number of features.\n",
    "        y : array-like, shape = [n_examples]\n",
    "          Target values.\n",
    "        \"\"\"  \n",
    "        X = bias_trick(X)\n",
    "        np.random.seed(self.random_state)\n",
    "        \n",
    "        if not self._theta:\n",
    "            self._theta = np.random.random(size=X.shape[1])\n",
    "            \n",
    "        h = self.sigmoid_func(X)\n",
    "        self._J_history.append(self.compute_cost(h, y))\n",
    "        \n",
    "        for n in range(self.n_iter):\n",
    "            self._theta -= self.eta * self.gradient(X, y, h)\n",
    "            h = self.sigmoid_func(X)\n",
    "            self._J_history.append(self.compute_cost(h, y))\n",
    "            if self._J_history[-2] - self._J_history[-1] < self.eps:\n",
    "                break\n",
    "                \n",
    "    @property\n",
    "    def J_history(self):\n",
    "        return self._J_history\n",
    "                \n",
    "    @property\n",
    "    def theta(self):\n",
    "        return self._theta\n",
    "    \n",
    "    def predict(self, x, threshold = 0.5):\n",
    "        \"\"\"Return the predicted class label\"\"\"\n",
    "        return 1 if self.sigmoid_func(x) >= threshold else 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "(5 points)\n",
    "\n",
    "Use 5-fold cross validation in order to find the best eps and eta params from the given lists.\n",
    "\n",
    "Shuffle the training set before you split the data to the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#### Your code here ####\n",
    "etas = [0.05, 0.005, 0.0005, 0.00005, 0.000005]\n",
    "epss = [0.01, 0.001, 0.0001, 0.00001, 0.000001]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val(X_train, y_train, logistic_regression_classifier, k = 5):\n",
    "    fold_indices = np.arange(X_train.shape[0])\n",
    "    np.random.shuffle(fold_indices)\n",
    "    eval_indices = np.array_split(fold_indices, k)\n",
    "    for indices in eval_indices:\n",
    "        logistic_regression_classifier.fit(X_train[indices], y_train[indices])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(X_train, y_train, X_test, y_test, epss, etas):\n",
    "    param_dict = {}\n",
    "    ###########################################################################\n",
    "    # TODO: Implement the function.                                           #\n",
    "    ###########################################################################\n",
    "    \n",
    "    # Perform the grid search\n",
    "    for eps in epss:\n",
    "        for eta in etas:\n",
    "            lr = LogisticRegressionGD(eps=eps, eta=eta)\n",
    "            cross_val(X_train, y_train, lr)\n",
    "            param_dict[(eps, eta)] = dict(accuracy = compute_accuracy(X_test, y_test, lr), history = lr.J_history)\n",
    "    ###########################################################################\n",
    "    #                             END OF YOUR CODE                            #\n",
    "    ###########################################################################\n",
    "    return param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = grid_search(X_training, y_training, X_test, y_test, epss, etas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1e-06, 5e-05),\n",
       " {'accuracy': 0.722,\n",
       "  'history': [1.8350060801285712,\n",
       "   1.7631359596333505,\n",
       "   1.6955904201092635,\n",
       "   1.6323334011569808,\n",
       "   1.5733486322390746,\n",
       "   1.5186330682076556,\n",
       "   1.4681858648540027,\n",
       "   1.4219943575216498,\n",
       "   1.3800193045604998,\n",
       "   1.3421820243709204,\n",
       "   1.3083558131641047,\n",
       "   1.2783631873417967,\n",
       "   1.2519792703918375,\n",
       "   1.2289404055705628,\n",
       "   1.2089561851636466,\n",
       "   1.191722753462922,\n",
       "   1.176935451633427,\n",
       "   1.1642994416594323,\n",
       "   1.1535376267597963,\n",
       "   1.1443957774682603,\n",
       "   1.1366451744008066,\n",
       "   1.13008327990733,\n",
       "   1.124532994767691,\n",
       "   1.1198410027903498,\n",
       "   1.1158756081717953,\n",
       "   1.1125243638016113,\n",
       "   1.1096916931942327,\n",
       "   1.107296632378173,\n",
       "   1.105270761746032,\n",
       "   1.1035563589737103,\n",
       "   1.10210477891105,\n",
       "   1.100875051154552,\n",
       "   1.099832677751377,\n",
       "   1.0989486097840808,\n",
       "   1.0981983807359679,\n",
       "   1.097561375357448,\n",
       "   1.0970202144627594,\n",
       "   1.096560238186744,\n",
       "   1.0961690724207156,\n",
       "   1.0958362652518385,\n",
       "   1.0955529821619308,\n",
       "   1.0953117504608114,\n",
       "   1.0951062449286062,\n",
       "   1.0949311079302952,\n",
       "   1.0947817983625157,\n",
       "   1.0946544647190433,\n",
       "   1.0945458383397573,\n",
       "   1.0944531435595102,\n",
       "   1.0943740220173488,\n",
       "   1.0943064688400255,\n",
       "   1.0942487787913013,\n",
       "   1.094199500792805,\n",
       "   1.0941573994837004,\n",
       "   1.0941214227040879,\n",
       "   1.0940906739683118,\n",
       "   1.0940643891453958,\n",
       "   1.0940419166897823,\n",
       "   1.0940227008706898,\n",
       "   1.0940062675362592,\n",
       "   1.0939922120221226,\n",
       "   1.093980188875548,\n",
       "   1.0939699031178793,\n",
       "   1.0939611028112461,\n",
       "   1.09395357273186,\n",
       "   1.0939471289827674,\n",
       "   1.0939416144046472,\n",
       "   1.0939368946649088,\n",
       "   1.093932854923614,\n",
       "   1.0939293969901809,\n",
       "   1.0939264368978487,\n",
       "   1.093923902833908,\n",
       "   1.093921733373022,\n",
       "   1.0939198759688689,\n",
       "   1.0939182856660183,\n",
       "   1.0939169239996431,\n",
       "   1.0939157580554761,\n",
       "   1.0939147596665193,\n",
       "   1.9128280062690979,\n",
       "   1.855504862513564,\n",
       "   1.8015540947879665,\n",
       "   1.7509153847430081,\n",
       "   1.703546580489619,\n",
       "   1.6594181856517156,\n",
       "   1.6185059898519103,\n",
       "   1.5807825753689146,\n",
       "   1.5462086736451115,\n",
       "   1.514725468038108,\n",
       "   1.4862488775678377,\n",
       "   1.4606665854838283,\n",
       "   1.4378381375842324,\n",
       "   1.4175979310363416,\n",
       "   1.3997604735710494,\n",
       "   1.384127020280851,\n",
       "   1.37049263452297,\n",
       "   1.3586528457567635,\n",
       "   1.3484093192269424,\n",
       "   1.339574227409596,\n",
       "   1.3319732555858093,\n",
       "   1.3254473486095761,\n",
       "   1.3198534060306861,\n",
       "   1.315064169792743,\n",
       "   1.310967541936776,\n",
       "   1.307465538084702,\n",
       "   1.3044730407854837,\n",
       "   1.301916474792102,\n",
       "   1.2997324892754047,\n",
       "   1.2978667019783816,\n",
       "   1.296272537540703,\n",
       "   1.2949101758526302,\n",
       "   1.2937456151268876,\n",
       "   1.2927498471805097,\n",
       "   1.291898138116563,\n",
       "   1.291169405314899,\n",
       "   1.2905456807146631,\n",
       "   1.2900116503110701,\n",
       "   1.2895542602503702,\n",
       "   1.289162380654601,\n",
       "   1.2888265191861041,\n",
       "   1.2885385772720994,\n",
       "   1.2882916427921376,\n",
       "   1.2880798138525567,\n",
       "   1.2878980490160012,\n",
       "   1.2877420400152175,\n",
       "   1.2876081035600253,\n",
       "   1.287493089349532,\n",
       "   1.2873943018351621,\n",
       "   1.2873094336515079,\n",
       "   1.2872365089489204,\n",
       "   1.287173835131344,\n",
       "   1.287119961731678,\n",
       "   1.287073645350834,\n",
       "   1.287033819750762,\n",
       "   1.2869995703305286,\n",
       "   1.286970112331926,\n",
       "   1.2869447722203344,\n",
       "   1.2869229717705002,\n",
       "   1.2869042144578737,\n",
       "   1.286888073816235,\n",
       "   1.286874183473188,\n",
       "   1.2868622286181832,\n",
       "   1.2868519386942479,\n",
       "   1.2868430811355658,\n",
       "   1.2868354559993411,\n",
       "   1.2868288913626884,\n",
       "   1.2868232393742742,\n",
       "   1.2868183728665685,\n",
       "   1.286814182448282,\n",
       "   1.286810574008267,\n",
       "   1.2868074665721243,\n",
       "   1.2868047904612427,\n",
       "   1.286802485711251,\n",
       "   1.286800500713054,\n",
       "   1.2867987910448972,\n",
       "   1.2867973184684276,\n",
       "   1.2867960500655824,\n",
       "   1.2867949574964257,\n",
       "   1.2867940163608984,\n",
       "   1.988845349016143,\n",
       "   1.896783646203952,\n",
       "   1.810841767931346,\n",
       "   1.7309753861785055,\n",
       "   1.657181551727266,\n",
       "   1.589480570948108,\n",
       "   1.527888074237257,\n",
       "   1.4723821500387617,\n",
       "   1.422872286300571,\n",
       "   1.3791771693550787,\n",
       "   1.3410165218962886,\n",
       "   1.3080184738037397,\n",
       "   1.2797398483623423,\n",
       "   1.2556938858742992,\n",
       "   1.235379244400874,\n",
       "   1.218305368185478,\n",
       "   1.2040114949469662,\n",
       "   1.192078663037373,\n",
       "   1.1821355057786942,\n",
       "   1.173859290706269,\n",
       "   1.1669737607743296,\n",
       "   1.1612451154188341,\n",
       "   1.1564771349040175,\n",
       "   1.1525061237379255,\n",
       "   1.1491960825484155,\n",
       "   1.1464343248504902,\n",
       "   1.1441276272301077,\n",
       "   1.142198923411382,\n",
       "   1.1405845094156137,\n",
       "   1.1392317063756032,\n",
       "   1.1380969206839184,\n",
       "   1.1371440420499281,\n",
       "   1.1363431249040195,\n",
       "   1.13566930511109,\n",
       "   1.1351019108213933,\n",
       "   1.1346237327987274,\n",
       "   1.1342204254011932,\n",
       "   1.1338800144405186,\n",
       "   1.13359249242037,\n",
       "   1.1333494852173955,\n",
       "   1.133143977208877,\n",
       "   1.132970084260066,\n",
       "   1.1328228659493227,\n",
       "   1.132698170007267,\n",
       "   1.1325925032437223,\n",
       "   1.132502924288962,\n",
       "   1.132426954329907,\n",
       "   1.1323625027152509,\n",
       "   1.132307804866803,\n",
       "   1.1322613703926045,\n",
       "   1.1322219396706563,\n",
       "   1.132188447476715,\n",
       "   1.1321599924785481,\n",
       "   1.1321358116228923,\n",
       "   1.1321152586085474,\n",
       "   1.132097785776443,\n",
       "   1.132082928860629,\n",
       "   1.1320702941374263,\n",
       "   1.1320595475870576,\n",
       "   1.132050405745857,\n",
       "   1.1320426279800495,\n",
       "   1.1320360099559952,\n",
       "   1.1320303781183128,\n",
       "   1.132025585017707,\n",
       "   1.132021505355699,\n",
       "   1.132018032634637,\n",
       "   1.1320150763190886,\n",
       "   1.1320125594295485,\n",
       "   1.1320104165018288,\n",
       "   1.132008591855939,\n",
       "   1.1320070381270275,\n",
       "   1.1320057150183251,\n",
       "   1.132004588242242,\n",
       "   1.1320036286209878,\n",
       "   1.8701282559844252,\n",
       "   1.811282540227247,\n",
       "   1.755755394128877,\n",
       "   1.7035026671821674,\n",
       "   1.6544980589047327,\n",
       "   1.6087285958964117,\n",
       "   1.5661879547074509,\n",
       "   1.5268681976436111,\n",
       "   1.490750786915268,\n",
       "   1.4577979663355978,\n",
       "   1.4279456693758084,\n",
       "   1.4010989670765333,\n",
       "   1.3771307023583097,\n",
       "   1.3558834361686871,\n",
       "   1.3371742841967023,\n",
       "   1.3208017935656036,\n",
       "   1.3065537975514332,\n",
       "   1.2942152154617839,\n",
       "   1.2835749815818582,\n",
       "   1.2744315974676232,\n",
       "   1.266597112409068,\n",
       "   1.2598995838206586,\n",
       "   1.2541842267955248,\n",
       "   1.2493135345327722,\n",
       "   1.2451666596525643,\n",
       "   1.2416383153638053,\n",
       "   1.2386374062517986,\n",
       "   1.2360855458394373,\n",
       "   1.2339155704828164,\n",
       "   1.2320701202673212,\n",
       "   1.230500328034906,\n",
       "   1.229164636565378,\n",
       "   1.2280277496870184,\n",
       "   1.2270597140580077,\n",
       "   1.226235123082739,\n",
       "   1.2255324317337308,\n",
       "   1.224933370054559,\n",
       "   1.2244224431834028,\n",
       "   1.2239865064176734,\n",
       "   1.2236144048417261,\n",
       "   1.2232966681711586,\n",
       "   1.223025252611215,\n",
       "   1.2227933226153624,\n",
       "   1.2225950664274736,\n",
       "   1.2224255401822424,\n",
       "   1.2222805361208167,\n",
       "   1.222156471156932,\n",
       "   1.222050292611456,\n",
       "   1.2219593984303705,\n",
       "   1.2218815696232495,\n",
       "   1.2218149130162836,\n",
       "   1.221757812715092,\n",
       "   1.2217088889262004,\n",
       "   1.2216669629994201,\n",
       "   1.221631027732677,\n",
       "   1.2216002221315139,\n",
       "   1.2215738099420874,\n",
       "   1.2215511613828547,\n",
       "   1.221531737589575,\n",
       "   1.2215150773634758,\n",
       "   1.2215007858757263,\n",
       "   1.2214885250346683,\n",
       "   1.2214780052671983,\n",
       "   1.2214689785035728,\n",
       "   1.2214612321869118,\n",
       "   1.22145458415569,\n",
       "   1.2214488782703636,\n",
       "   1.221443980674609,\n",
       "   1.2214397765980287,\n",
       "   1.2214361676210568,\n",
       "   1.221433069334564,\n",
       "   1.2214304093366593,\n",
       "   1.2214281255176638,\n",
       "   1.2214261645914548,\n",
       "   1.2214244808375028,\n",
       "   1.2214230350231499,\n",
       "   1.2214217934801317,\n",
       "   1.2214207273131137,\n",
       "   1.2214198117212614,\n",
       "   1.952209188404928,\n",
       "   1.8760840493522712,\n",
       "   1.8042363000578956,\n",
       "   1.736714085288689,\n",
       "   1.6735919590219697,\n",
       "   1.6149596243195237,\n",
       "   1.5609048199291033,\n",
       "   1.5114922426061952,\n",
       "   1.4667417234933016,\n",
       "   1.4266096575050187,\n",
       "   1.3909774494448146,\n",
       "   1.3596493562553627,\n",
       "   1.3323599509270718,\n",
       "   1.3087892759740356,\n",
       "   1.2885823510536238,\n",
       "   1.2713694128182254,\n",
       "   1.2567839484368366,\n",
       "   1.2444767679526982,\n",
       "   1.234125550839604,\n",
       "   1.225440185160848,\n",
       "   1.2181647018538453,\n",
       "   1.2120767533336303,\n",
       "   1.2069855158564429,\n",
       "   1.202728723276321,\n",
       "   1.199169345603102,\n",
       "   1.196192252399848,\n",
       "   1.193701065153473,\n",
       "   1.1916153054258047,\n",
       "   1.1898678808998704,\n",
       "   1.1884029114708485,\n",
       "   1.1871738749321528,\n",
       "   1.1861420406835537,\n",
       "   1.1852751559258963,\n",
       "   1.184546349036414,\n",
       "   1.1839332172946657,\n",
       "   1.183417069634355,\n",
       "   1.1829822989051237,\n",
       "   1.182615861834632,\n",
       "   1.1823068482785175,\n",
       "   1.1820461243482432,\n",
       "   1.1818260365976285,\n",
       "   1.1816401666484326,\n",
       "   1.1814831274818243,\n",
       "   1.1813503941604515,\n",
       "   1.1812381630196718,\n",
       "   1.181143234417695,\n",
       "   1.1810629149997545,\n",
       "   1.1809949361427166,\n",
       "   1.1809373858307999,\n",
       "   1.1808886516928776,\n",
       "   1.1808473733259575,\n",
       "   1.1808124023533733,\n",
       "   1.1807827689326666,\n",
       "   1.1807576536475395,\n",
       "   1.180736363899093,\n",
       "   1.180718314060839,\n",
       "   1.1807030087852823,\n",
       "   1.180690028951943,\n",
       "   1.1806790198312294,\n",
       "   1.180669681108714,\n",
       "   1.1806617584726382,\n",
       "   1.1806550365159234,\n",
       "   1.1806493327443348,\n",
       "   1.1806444925160797,\n",
       "   1.1806403847662152,\n",
       "   1.1806368983927102,\n",
       "   1.1806339392006282,\n",
       "   1.1806314273173535,\n",
       "   1.1806292950055441,\n",
       "   1.1806274848120648,\n",
       "   1.1806259480008523,\n",
       "   1.1806246432258243,\n",
       "   1.1806235354067929,\n",
       "   1.180622594777125]})"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_parameters = sorted(param_dict.items(), key=lambda entry: entry[1][\"accuracy\"])\n",
    "best_parameters[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal distribution pdf\n",
    "\n",
    "(2 Points)\n",
    "\n",
    "Implement the normal distribution pdf \n",
    "$$\n",
    "f(x;\\mu,\\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\big{(}\\frac{x-\\mu}{\\sigma}\\big{)}^2}\n",
    "$$\n",
    "Write an efficient vectorized code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Your code here ####\n",
    "# calc normal pdf    \n",
    "def norm_pdf(data, mu, sigma):\n",
    "    if mu == 0 and sigma == 1:\n",
    "        # Optimize for a normalized distribution\n",
    "        pdf = np.power(np.e, -np.square(x) / 2) / np.sqrt(2 * np.pi)\n",
    "    else:\n",
    "        pdf = np.power(np.e, -np.square((x - mu) / sigma) / 2) / (np.sqrt(2*np.pi)\n",
    "    return pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation Maximization\n",
    "\n",
    "(15 Points)\n",
    "\n",
    "Implement the Expectation Maximization algorithm for gaussian mixture model.\n",
    "\n",
    "The class should hold the distribution params.\n",
    "\n",
    "Use -log likelihood as the cost function:\n",
    "$$\n",
    "cost(x) = \\sum_{d=1}^{m}-log(w * pdf(x; \\mu, \\sigma))\n",
    "$$\n",
    "\n",
    "\\* The above is the cost of one gaussian. Think how to use the cost function for gaussian mixture.\n",
    "\n",
    "Your class should contain the following functions:\n",
    "1. init_params - initialize distribution params\n",
    "1. expectation - calculating responsibilities\n",
    "1. maximization - updating distribution params\n",
    "1. fit - the learning function\n",
    "1. get_dist_params - return the distribution params\n",
    "\n",
    "\\* You can add more functions if you think this is necessary\n",
    "\n",
    "Don't change the eps params (eps=0.01)\n",
    "\n",
    "When you need to calculate the pdf of a normal distribution use the function `norm_pdf` that you implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EM(object):\n",
    "    \"\"\"\n",
    "    Naive Bayes Classifier using Gauusian Mixture Model (EM) for calculating the likelihood.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    k : int\n",
    "      Number of gaussians in each dimension\n",
    "    n_iter : int\n",
    "      Passes over the training dataset in the EM proccess\n",
    "    eps: float\n",
    "      minimal change in the cost to declare convergence\n",
    "    random_state : int\n",
    "      Random number generator seed for random params initialization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k=1, n_iter=1000, eps=0.01):\n",
    "        self.k = k\n",
    "        self.n_iter = n_iter\n",
    "        self.eps = eps\n",
    "\n",
    "    # initial guesses for parameters\n",
    "    def init_params(self, data):\n",
    "        \"\"\"\n",
    "        Initialize distribution params\n",
    "        \"\"\"\n",
    "        pass\n",
    "    def expectation(self, data):\n",
    "        \"\"\"\n",
    "        E step - calculating responsibilities\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def maximization(self, data):\n",
    "        \"\"\"\n",
    "        M step - updating distribution params\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def fit(self, data):\n",
    "        \"\"\" \n",
    "        Fit training data (the learning phase).\n",
    "        Use init_params and then expectation and maximization function in order to find params \n",
    "        for the distribution. \n",
    "        Store the params in attributes of the EM object.\n",
    "        Stop the function when the difference between the previous cost and the current is less than eps\n",
    "        or when you reach n_iter.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def get_dist_params(self):\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "(15 Points)\n",
    "\n",
    "Implement the Naive Bayes algorithm.\n",
    "\n",
    "For calculating the likelihood use the EM algorithm that you implemented above to find the distribution params. With these params you can calculate the likelihood probability.\n",
    "\n",
    "Calculate the prior probability directly from the training set.\n",
    "\n",
    "Your class should contain the following functions:\n",
    "1. fit - the learning function\n",
    "1. predict - the function for predicting an instance (or instances) after the fit function was executed\n",
    "\n",
    "\\* You can add more functions if you think this is necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesGaussian(object):\n",
    "    \"\"\"\n",
    "    Naive Bayes Classifier using Gauusian Mixture Model (EM) for calculating the likelihood.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    k : int\n",
    "      Number of gaussians in each dimension\n",
    "    random_state : int\n",
    "      Random number generator seed for random params initialization.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, k=1):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\" \n",
    "        Fit training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_examples, n_features]\n",
    "          Training vectors, where n_examples is the number of examples and\n",
    "          n_features is the number of features.\n",
    "        y : array-like, shape = [n_examples]\n",
    "          Target values.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Return the predicted class label\"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "(20 points)\n",
    "\n",
    "In this section you will build 2 models and fit them to 2 datasets\n",
    "\n",
    "First 1000 training points and first 500 test points:\n",
    "1. Use the first 1000 points from the training set (take the first original 1000 points - before the shuffle) and the first 500 points from the test set.\n",
    "1. Fit Logistic Regression model with the best params you found earlier.\n",
    "1. Fit Naive Bayes model. Remember that you need to select the number of gaussians in the EM.\n",
    "1. Print the training and test accuracies for each model.\n",
    "1. Use the `plot_decision_regions` function to plot the decision boundaries for each model (for this you need to use the training set as the input)\n",
    "1. Plot the cost Vs the iteration number for the Logistic Regression model\n",
    "\n",
    "Use all the training set points:\n",
    "1. Repeat sections 2-6 for all the training set points\n",
    "1. Provide one or two sentences on each graph explaining what you observe in the graph.\n",
    "\n",
    "#### Don't forget to label your graphs ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Your code here ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open question\n",
    "\n",
    "(20 points) \n",
    "\n",
    "1. In this homework we explored two types of models: Naive Bayes using EM, and Logistic regression.  \n",
    "    - Generate one dataset that you think Naive Bayes will work better than Logisitc Regression.\n",
    "    - Generate another dataset that you think Logistic Regression will work better than Naive Bayes using EM.\n",
    "    - Explain the reasoning behind each dataset.  \n",
    "\n",
    "(The number of features and instances is up to you, but use only 2 classes)\n",
    "\n",
    "2. Visualize the datasets like in the beginning of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your code and explanations here ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}